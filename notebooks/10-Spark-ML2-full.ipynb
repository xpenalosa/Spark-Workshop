{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark ML - Part Two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark ML library was introduced because MLlib (RDD-based library you used in Part One) wasn’t scalable and extendable enough, nor was it sufficiently practical for use in real machine learning projects. The goal of the new, now official Spark's machine learning library, is to generalize machine learning operations and streamline machine learning processes. Influenced by the Python’s scikit-learn library, it introduces several new abstractions - estimators, transformers, and evaluators — that can be combined to form pipelines. All four can be parameterized with ML parameters in a general way.\n",
    "\n",
    "Spark ML ubiquitously uses DataFrame objects to present datasets. This is why the old MLlib algorithms can’t be simply upgraded: the Spark ML architecture requires structural changes, so new implementations of the same algorithms are necessary. \n",
    "\n",
    "In this notebook you will use Spark ML library to perform classification and clustering using logistic regression, decision trees, random forests and k-means clustering algorithms.\n",
    "\n",
    "Before anything else, let's initialize a Spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark course - ML Part 2\").\\\n",
    "    master(\"local[*]\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression - preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example dataset that you’ll use for logistic regression is the well-known adult dataset (http://archive.ics.uci.edu/ml/datasets/Adult), extracted from the 1994 United States census data. It contains 13 attributes with data about a person’s sex, age, education, marital status, race, native country, and so on, and the target variable (income). The goal is to predict whether a person earns more or less than $50,000 per year (the income column contains only values 1 and 0).\n",
    "\n",
    "To load the data execute the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def tofloat(s):\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def rawtorow(s):\n",
    "    return Row(float(s[0]), s[1], float(s[2]), s[3], s[4], s[5], s[6], s[7], s[8], \n",
    "               float(s[9]), float(s[10]), float(s[11]), s[12], s[13])\n",
    "\n",
    "dfraw = sc.textFile(\"../first-edition/ch08/adult.raw\", 4).\\\n",
    "    map(lambda x: x.split(\", \")).map(rawtorow).toDF(['age', 'workclass', 'fnlwgt', 'education', \n",
    "                                                    'marital_status', 'occupation', 'relationship',\n",
    "                                                   'race', 'sex', 'capital_gain', 'capital_loss', \n",
    "                                                    'hours_per_week', 'native_country', 'income'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the first 200 rows of the data using the `show` method and specifying 200 as the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+--------+------------+--------------------+-----------------+--------------+------------------+------+------------+------------+--------------+--------------+------+\n",
      "| age|       workclass|  fnlwgt|   education|      marital_status|       occupation|  relationship|              race|   sex|capital_gain|capital_loss|hours_per_week|native_country|income|\n",
      "+----+----------------+--------+------------+--------------------+-----------------+--------------+------------------+------+------------+------------+--------------+--------------+------+\n",
      "|39.0|       State-gov| 77516.0|   Bachelors|       Never-married|     Adm-clerical| Not-in-family|             White|  Male|      2174.0|         0.0|          40.0| United-States| <=50K|\n",
      "|50.0|Self-emp-not-inc| 83311.0|   Bachelors|  Married-civ-spouse|  Exec-managerial|       Husband|             White|  Male|         0.0|         0.0|          13.0| United-States| <=50K|\n",
      "|38.0|         Private|215646.0|     HS-grad|            Divorced|Handlers-cleaners| Not-in-family|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|53.0|         Private|234721.0|        11th|  Married-civ-spouse|Handlers-cleaners|       Husband|             Black|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|28.0|         Private|338409.0|   Bachelors|  Married-civ-spouse|   Prof-specialty|          Wife|             Black|Female|         0.0|         0.0|          40.0|          Cuba| <=50K|\n",
      "|37.0|         Private|284582.0|     Masters|  Married-civ-spouse|  Exec-managerial|          Wife|             White|Female|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|49.0|         Private|160187.0|         9th|Married-spouse-ab...|    Other-service| Not-in-family|             Black|Female|         0.0|         0.0|          16.0|       Jamaica| <=50K|\n",
      "|52.0|Self-emp-not-inc|209642.0|     HS-grad|  Married-civ-spouse|  Exec-managerial|       Husband|             White|  Male|         0.0|         0.0|          45.0| United-States|  >50K|\n",
      "|31.0|         Private| 45781.0|     Masters|       Never-married|   Prof-specialty| Not-in-family|             White|Female|     14084.0|         0.0|          50.0| United-States|  >50K|\n",
      "|42.0|         Private|159449.0|   Bachelors|  Married-civ-spouse|  Exec-managerial|       Husband|             White|  Male|      5178.0|         0.0|          40.0| United-States|  >50K|\n",
      "|37.0|         Private|280464.0|Some-college|  Married-civ-spouse|  Exec-managerial|       Husband|             Black|  Male|         0.0|         0.0|          80.0| United-States|  >50K|\n",
      "|30.0|       State-gov|141297.0|   Bachelors|  Married-civ-spouse|   Prof-specialty|       Husband|Asian-Pac-Islander|  Male|         0.0|         0.0|          40.0|         India|  >50K|\n",
      "|23.0|         Private|122272.0|   Bachelors|       Never-married|     Adm-clerical|     Own-child|             White|Female|         0.0|         0.0|          30.0| United-States| <=50K|\n",
      "|32.0|         Private|205019.0|  Assoc-acdm|       Never-married|            Sales| Not-in-family|             Black|  Male|         0.0|         0.0|          50.0| United-States| <=50K|\n",
      "|40.0|         Private|121772.0|   Assoc-voc|  Married-civ-spouse|     Craft-repair|       Husband|Asian-Pac-Islander|  Male|         0.0|         0.0|          40.0|             ?|  >50K|\n",
      "|34.0|         Private|245487.0|     7th-8th|  Married-civ-spouse| Transport-moving|       Husband|Amer-Indian-Eskimo|  Male|         0.0|         0.0|          45.0|        Mexico| <=50K|\n",
      "|25.0|Self-emp-not-inc|176756.0|     HS-grad|       Never-married|  Farming-fishing|     Own-child|             White|  Male|         0.0|         0.0|          35.0| United-States| <=50K|\n",
      "|32.0|         Private|186824.0|     HS-grad|       Never-married|Machine-op-inspct|     Unmarried|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|38.0|         Private| 28887.0|        11th|  Married-civ-spouse|            Sales|       Husband|             White|  Male|         0.0|         0.0|          50.0| United-States| <=50K|\n",
      "|43.0|Self-emp-not-inc|292175.0|     Masters|            Divorced|  Exec-managerial|     Unmarried|             White|Female|         0.0|         0.0|          45.0| United-States|  >50K|\n",
      "|40.0|         Private|193524.0|   Doctorate|  Married-civ-spouse|   Prof-specialty|       Husband|             White|  Male|         0.0|         0.0|          60.0| United-States|  >50K|\n",
      "|54.0|         Private|302146.0|     HS-grad|           Separated|    Other-service|     Unmarried|             Black|Female|         0.0|         0.0|          20.0| United-States| <=50K|\n",
      "|35.0|     Federal-gov| 76845.0|         9th|  Married-civ-spouse|  Farming-fishing|       Husband|             Black|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|43.0|         Private|117037.0|        11th|  Married-civ-spouse| Transport-moving|       Husband|             White|  Male|         0.0|      2042.0|          40.0| United-States| <=50K|\n",
      "|59.0|         Private|109015.0|     HS-grad|            Divorced|     Tech-support|     Unmarried|             White|Female|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|56.0|       Local-gov|216851.0|   Bachelors|  Married-civ-spouse|     Tech-support|       Husband|             White|  Male|         0.0|         0.0|          40.0| United-States|  >50K|\n",
      "|19.0|         Private|168294.0|     HS-grad|       Never-married|     Craft-repair|     Own-child|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|54.0|               ?|180211.0|Some-college|  Married-civ-spouse|                ?|       Husband|Asian-Pac-Islander|  Male|         0.0|         0.0|          60.0|         South|  >50K|\n",
      "|39.0|         Private|367260.0|     HS-grad|            Divorced|  Exec-managerial| Not-in-family|             White|  Male|         0.0|         0.0|          80.0| United-States| <=50K|\n",
      "|49.0|         Private|193366.0|     HS-grad|  Married-civ-spouse|     Craft-repair|       Husband|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|23.0|       Local-gov|190709.0|  Assoc-acdm|       Never-married|  Protective-serv| Not-in-family|             White|  Male|         0.0|         0.0|          52.0| United-States| <=50K|\n",
      "|20.0|         Private|266015.0|Some-college|       Never-married|            Sales|     Own-child|             Black|  Male|         0.0|         0.0|          44.0| United-States| <=50K|\n",
      "|45.0|         Private|386940.0|   Bachelors|            Divorced|  Exec-managerial|     Own-child|             White|  Male|         0.0|      1408.0|          40.0| United-States| <=50K|\n",
      "|30.0|     Federal-gov| 59951.0|Some-college|  Married-civ-spouse|     Adm-clerical|     Own-child|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|22.0|       State-gov|311512.0|Some-college|  Married-civ-spouse|    Other-service|       Husband|             Black|  Male|         0.0|         0.0|          15.0| United-States| <=50K|\n",
      "|48.0|         Private|242406.0|        11th|       Never-married|Machine-op-inspct|     Unmarried|             White|  Male|         0.0|         0.0|          40.0|   Puerto-Rico| <=50K|\n",
      "|21.0|         Private|197200.0|Some-college|       Never-married|Machine-op-inspct|     Own-child|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|19.0|         Private|544091.0|     HS-grad|   Married-AF-spouse|     Adm-clerical|          Wife|             White|Female|         0.0|         0.0|          25.0| United-States| <=50K|\n",
      "|31.0|         Private| 84154.0|Some-college|  Married-civ-spouse|            Sales|       Husband|             White|  Male|         0.0|         0.0|          38.0|             ?|  >50K|\n",
      "|48.0|Self-emp-not-inc|265477.0|  Assoc-acdm|  Married-civ-spouse|   Prof-specialty|       Husband|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|31.0|         Private|507875.0|         9th|  Married-civ-spouse|Machine-op-inspct|       Husband|             White|  Male|         0.0|         0.0|          43.0| United-States| <=50K|\n",
      "|53.0|Self-emp-not-inc| 88506.0|   Bachelors|  Married-civ-spouse|   Prof-specialty|       Husband|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|24.0|         Private|172987.0|   Bachelors|  Married-civ-spouse|     Tech-support|       Husband|             White|  Male|         0.0|         0.0|          50.0| United-States| <=50K|\n",
      "|49.0|         Private| 94638.0|     HS-grad|           Separated|     Adm-clerical|     Unmarried|             White|Female|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|25.0|         Private|289980.0|     HS-grad|       Never-married|Handlers-cleaners| Not-in-family|             White|  Male|         0.0|         0.0|          35.0| United-States| <=50K|\n",
      "|57.0|     Federal-gov|337895.0|   Bachelors|  Married-civ-spouse|   Prof-specialty|       Husband|             Black|  Male|         0.0|         0.0|          40.0| United-States|  >50K|\n",
      "|53.0|         Private|144361.0|     HS-grad|  Married-civ-spouse|Machine-op-inspct|       Husband|             White|  Male|         0.0|         0.0|          38.0| United-States| <=50K|\n",
      "|44.0|         Private|128354.0|     Masters|            Divorced|  Exec-managerial|     Unmarried|             White|Female|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|41.0|       State-gov|101603.0|   Assoc-voc|  Married-civ-spouse|     Craft-repair|       Husband|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|29.0|         Private|271466.0|   Assoc-voc|       Never-married|   Prof-specialty| Not-in-family|             White|  Male|         0.0|         0.0|          43.0| United-States| <=50K|\n",
      "|25.0|         Private| 32275.0|Some-college|  Married-civ-spouse|  Exec-managerial|          Wife|             Other|Female|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|18.0|         Private|226956.0|     HS-grad|       Never-married|    Other-service|     Own-child|             White|Female|         0.0|         0.0|          30.0|             ?| <=50K|\n",
      "|47.0|         Private| 51835.0| Prof-school|  Married-civ-spouse|   Prof-specialty|          Wife|             White|Female|         0.0|      1902.0|          60.0|      Honduras|  >50K|\n",
      "|50.0|     Federal-gov|251585.0|   Bachelors|            Divorced|  Exec-managerial| Not-in-family|             White|  Male|         0.0|         0.0|          55.0| United-States|  >50K|\n",
      "|47.0|    Self-emp-inc|109832.0|     HS-grad|            Divorced|  Exec-managerial| Not-in-family|             White|  Male|         0.0|         0.0|          60.0| United-States| <=50K|\n",
      "|43.0|         Private|237993.0|Some-college|  Married-civ-spouse|     Tech-support|       Husband|             White|  Male|         0.0|         0.0|          40.0| United-States|  >50K|\n",
      "|46.0|         Private|216666.0|     5th-6th|  Married-civ-spouse|Machine-op-inspct|       Husband|             White|  Male|         0.0|         0.0|          40.0|        Mexico| <=50K|\n",
      "|35.0|         Private| 56352.0|   Assoc-voc|  Married-civ-spouse|    Other-service|       Husband|             White|  Male|         0.0|         0.0|          40.0|   Puerto-Rico| <=50K|\n",
      "|41.0|         Private|147372.0|     HS-grad|  Married-civ-spouse|     Adm-clerical|       Husband|             White|  Male|         0.0|         0.0|          48.0| United-States| <=50K|\n",
      "|30.0|         Private|188146.0|     HS-grad|  Married-civ-spouse|Machine-op-inspct|       Husband|             White|  Male|      5013.0|         0.0|          40.0| United-States| <=50K|\n",
      "|30.0|         Private| 59496.0|   Bachelors|  Married-civ-spouse|            Sales|       Husband|             White|  Male|      2407.0|         0.0|          40.0| United-States| <=50K|\n",
      "|32.0|               ?|293936.0|     7th-8th|Married-spouse-ab...|                ?| Not-in-family|             White|  Male|         0.0|         0.0|          40.0|             ?| <=50K|\n",
      "|48.0|         Private|149640.0|     HS-grad|  Married-civ-spouse| Transport-moving|       Husband|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|42.0|         Private|116632.0|   Doctorate|  Married-civ-spouse|   Prof-specialty|       Husband|             White|  Male|         0.0|         0.0|          45.0| United-States|  >50K|\n",
      "|29.0|         Private|105598.0|Some-college|            Divorced|     Tech-support| Not-in-family|             White|  Male|         0.0|         0.0|          58.0| United-States| <=50K|\n",
      "|36.0|         Private|155537.0|     HS-grad|  Married-civ-spouse|     Craft-repair|       Husband|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|28.0|         Private|183175.0|Some-college|            Divorced|     Adm-clerical| Not-in-family|             White|Female|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|53.0|         Private|169846.0|     HS-grad|  Married-civ-spouse|     Adm-clerical|          Wife|             White|Female|         0.0|         0.0|          40.0| United-States|  >50K|\n",
      "|49.0|    Self-emp-inc|191681.0|Some-college|  Married-civ-spouse|  Exec-managerial|       Husband|             White|  Male|         0.0|         0.0|          50.0| United-States|  >50K|\n",
      "|25.0|               ?|200681.0|Some-college|       Never-married|                ?|     Own-child|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|19.0|         Private|101509.0|Some-college|       Never-married|   Prof-specialty|     Own-child|             White|  Male|         0.0|         0.0|          32.0| United-States| <=50K|\n",
      "|31.0|         Private|309974.0|   Bachelors|           Separated|            Sales|     Own-child|             Black|Female|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|29.0|Self-emp-not-inc|162298.0|   Bachelors|  Married-civ-spouse|            Sales|       Husband|             White|  Male|         0.0|         0.0|          70.0| United-States|  >50K|\n",
      "|23.0|         Private|211678.0|Some-college|       Never-married|Machine-op-inspct| Not-in-family|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|79.0|         Private|124744.0|Some-college|  Married-civ-spouse|   Prof-specialty|Other-relative|             White|  Male|         0.0|         0.0|          20.0| United-States| <=50K|\n",
      "|27.0|         Private|213921.0|     HS-grad|       Never-married|    Other-service|     Own-child|             White|  Male|         0.0|         0.0|          40.0|        Mexico| <=50K|\n",
      "|40.0|         Private| 32214.0|  Assoc-acdm|  Married-civ-spouse|     Adm-clerical|       Husband|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|67.0|               ?|212759.0|        10th|  Married-civ-spouse|                ?|       Husband|             White|  Male|         0.0|         0.0|           2.0| United-States| <=50K|\n",
      "|18.0|         Private|309634.0|        11th|       Never-married|    Other-service|     Own-child|             White|Female|         0.0|         0.0|          22.0| United-States| <=50K|\n",
      "|31.0|       Local-gov|125927.0|     7th-8th|  Married-civ-spouse|  Farming-fishing|       Husband|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|18.0|         Private|446839.0|     HS-grad|       Never-married|            Sales| Not-in-family|             White|  Male|         0.0|         0.0|          30.0| United-States| <=50K|\n",
      "|52.0|         Private|276515.0|   Bachelors|  Married-civ-spouse|    Other-service|       Husband|             White|  Male|         0.0|         0.0|          40.0|          Cuba| <=50K|\n",
      "|46.0|         Private| 51618.0|     HS-grad|  Married-civ-spouse|    Other-service|          Wife|             White|Female|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|59.0|         Private|159937.0|     HS-grad|  Married-civ-spouse|            Sales|       Husband|             White|  Male|         0.0|         0.0|          48.0| United-States| <=50K|\n",
      "|44.0|         Private|343591.0|     HS-grad|            Divorced|     Craft-repair| Not-in-family|             White|Female|     14344.0|         0.0|          40.0| United-States|  >50K|\n",
      "|53.0|         Private|346253.0|     HS-grad|            Divorced|            Sales|     Own-child|             White|Female|         0.0|         0.0|          35.0| United-States| <=50K|\n",
      "|49.0|       Local-gov|268234.0|     HS-grad|  Married-civ-spouse|  Protective-serv|       Husband|             White|  Male|         0.0|         0.0|          40.0| United-States|  >50K|\n",
      "|33.0|         Private|202051.0|     Masters|  Married-civ-spouse|   Prof-specialty|       Husband|             White|  Male|         0.0|         0.0|          50.0| United-States| <=50K|\n",
      "|30.0|         Private| 54334.0|         9th|       Never-married|            Sales| Not-in-family|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|43.0|     Federal-gov|410867.0|   Doctorate|       Never-married|   Prof-specialty| Not-in-family|             White|Female|         0.0|         0.0|          50.0| United-States|  >50K|\n",
      "|57.0|         Private|249977.0|   Assoc-voc|  Married-civ-spouse|   Prof-specialty|       Husband|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|37.0|         Private|286730.0|Some-college|            Divorced|     Craft-repair|     Unmarried|             White|Female|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|28.0|         Private|212563.0|Some-college|            Divorced|Machine-op-inspct|     Unmarried|             Black|Female|         0.0|         0.0|          25.0| United-States| <=50K|\n",
      "|30.0|         Private|117747.0|     HS-grad|  Married-civ-spouse|            Sales|          Wife|Asian-Pac-Islander|Female|         0.0|      1573.0|          35.0|             ?| <=50K|\n",
      "|34.0|       Local-gov|226296.0|   Bachelors|  Married-civ-spouse|  Protective-serv|       Husband|             White|  Male|         0.0|         0.0|          40.0| United-States|  >50K|\n",
      "|29.0|       Local-gov|115585.0|Some-college|       Never-married|Handlers-cleaners| Not-in-family|             White|  Male|         0.0|         0.0|          50.0| United-States| <=50K|\n",
      "|48.0|Self-emp-not-inc|191277.0|   Doctorate|  Married-civ-spouse|   Prof-specialty|       Husband|             White|  Male|         0.0|      1902.0|          60.0| United-States|  >50K|\n",
      "|37.0|         Private|202683.0|Some-college|  Married-civ-spouse|            Sales|       Husband|             White|  Male|         0.0|         0.0|          48.0| United-States|  >50K|\n",
      "|48.0|         Private|171095.0|  Assoc-acdm|            Divorced|  Exec-managerial|     Unmarried|             White|Female|         0.0|         0.0|          40.0|       England| <=50K|\n",
      "|32.0|     Federal-gov|249409.0|     HS-grad|       Never-married|    Other-service|     Own-child|             Black|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|76.0|         Private|124191.0|     Masters|  Married-civ-spouse|  Exec-managerial|       Husband|             White|  Male|         0.0|         0.0|          40.0| United-States|  >50K|\n",
      "|44.0|         Private|198282.0|   Bachelors|  Married-civ-spouse|  Exec-managerial|       Husband|             White|  Male|     15024.0|         0.0|          60.0| United-States|  >50K|\n",
      "|47.0|Self-emp-not-inc|149116.0|     Masters|       Never-married|   Prof-specialty| Not-in-family|             White|Female|         0.0|         0.0|          50.0| United-States| <=50K|\n",
      "|20.0|         Private|188300.0|Some-college|       Never-married|     Tech-support|     Own-child|             White|Female|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|29.0|         Private|103432.0|     HS-grad|       Never-married|     Craft-repair| Not-in-family|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|32.0|    Self-emp-inc|317660.0|     HS-grad|  Married-civ-spouse|     Craft-repair|       Husband|             White|  Male|      7688.0|         0.0|          40.0| United-States|  >50K|\n",
      "|17.0|               ?|304873.0|        10th|       Never-married|                ?|     Own-child|             White|Female|     34095.0|         0.0|          32.0| United-States| <=50K|\n",
      "|30.0|         Private|194901.0|        11th|       Never-married|Handlers-cleaners|     Own-child|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|31.0|       Local-gov|189265.0|     HS-grad|       Never-married|     Adm-clerical| Not-in-family|             White|Female|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|42.0|         Private|124692.0|     HS-grad|  Married-civ-spouse|Handlers-cleaners|       Husband|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|24.0|         Private|432376.0|   Bachelors|       Never-married|            Sales|Other-relative|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|38.0|         Private| 65324.0| Prof-school|  Married-civ-spouse|   Prof-specialty|       Husband|             White|  Male|         0.0|         0.0|          40.0| United-States|  >50K|\n",
      "|56.0|Self-emp-not-inc|335605.0|     HS-grad|  Married-civ-spouse|    Other-service|       Husband|             White|  Male|         0.0|      1887.0|          50.0|        Canada|  >50K|\n",
      "|28.0|         Private|377869.0|Some-college|  Married-civ-spouse|            Sales|          Wife|             White|Female|      4064.0|         0.0|          25.0| United-States| <=50K|\n",
      "|36.0|         Private|102864.0|     HS-grad|       Never-married|Machine-op-inspct|     Own-child|             White|Female|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|53.0|         Private| 95647.0|         9th|  Married-civ-spouse|Handlers-cleaners|       Husband|             White|  Male|         0.0|         0.0|          50.0| United-States| <=50K|\n",
      "|56.0|    Self-emp-inc|303090.0|Some-college|  Married-civ-spouse|            Sales|       Husband|             White|  Male|         0.0|         0.0|          50.0| United-States| <=50K|\n",
      "|49.0|       Local-gov|197371.0|   Assoc-voc|  Married-civ-spouse|     Craft-repair|       Husband|             Black|  Male|         0.0|         0.0|          40.0| United-States|  >50K|\n",
      "|55.0|         Private|247552.0|Some-college|  Married-civ-spouse|            Sales|       Husband|             White|  Male|         0.0|         0.0|          56.0| United-States| <=50K|\n",
      "|22.0|         Private|102632.0|     HS-grad|       Never-married|     Craft-repair| Not-in-family|             White|  Male|         0.0|         0.0|          41.0| United-States| <=50K|\n",
      "|21.0|         Private|199915.0|Some-college|       Never-married|    Other-service|     Own-child|             White|Female|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|40.0|         Private|118853.0|   Bachelors|  Married-civ-spouse|  Exec-managerial|       Husband|             White|  Male|         0.0|         0.0|          60.0| United-States| <=50K|\n",
      "|30.0|         Private| 77143.0|   Bachelors|       Never-married|  Exec-managerial|     Own-child|             Black|  Male|         0.0|         0.0|          40.0|       Germany| <=50K|\n",
      "|29.0|       State-gov|267989.0|   Bachelors|  Married-civ-spouse|   Prof-specialty|       Husband|             White|  Male|         0.0|         0.0|          50.0| United-States|  >50K|\n",
      "|19.0|         Private|301606.0|Some-college|       Never-married|    Other-service|     Own-child|             Black|  Male|         0.0|         0.0|          35.0| United-States| <=50K|\n",
      "|47.0|         Private|287828.0|   Bachelors|  Married-civ-spouse|  Exec-managerial|          Wife|             White|Female|         0.0|         0.0|          40.0| United-States|  >50K|\n",
      "|20.0|         Private|111697.0|Some-college|       Never-married|     Adm-clerical|     Own-child|             White|Female|         0.0|      1719.0|          28.0| United-States| <=50K|\n",
      "|31.0|         Private|114937.0|  Assoc-acdm|  Married-civ-spouse|     Adm-clerical|       Husband|             White|  Male|         0.0|         0.0|          40.0| United-States|  >50K|\n",
      "|35.0|               ?|129305.0|     HS-grad|  Married-civ-spouse|                ?|       Husband|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|39.0|         Private|365739.0|Some-college|            Divorced|     Craft-repair| Not-in-family|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|28.0|         Private| 69621.0|  Assoc-acdm|       Never-married|            Sales| Not-in-family|             White|Female|         0.0|         0.0|          60.0| United-States| <=50K|\n",
      "|24.0|         Private| 43323.0|     HS-grad|       Never-married|    Other-service| Not-in-family|             White|Female|         0.0|      1762.0|          40.0| United-States| <=50K|\n",
      "|38.0|Self-emp-not-inc|120985.0|     HS-grad|  Married-civ-spouse|     Craft-repair|       Husband|             White|  Male|      4386.0|         0.0|          35.0| United-States| <=50K|\n",
      "|37.0|         Private|254202.0|   Bachelors|  Married-civ-spouse|            Sales|       Husband|             White|  Male|         0.0|         0.0|          50.0| United-States| <=50K|\n",
      "|46.0|         Private|146195.0|  Assoc-acdm|            Divorced|     Tech-support| Not-in-family|             Black|Female|         0.0|         0.0|          36.0| United-States| <=50K|\n",
      "|38.0|     Federal-gov|125933.0|     Masters|  Married-civ-spouse|   Prof-specialty|       Husband|             White|  Male|         0.0|         0.0|          40.0|          Iran|  >50K|\n",
      "|43.0|Self-emp-not-inc| 56920.0|     HS-grad|  Married-civ-spouse|     Craft-repair|       Husband|             White|  Male|         0.0|         0.0|          60.0| United-States| <=50K|\n",
      "|27.0|         Private|163127.0|   Assoc-voc|  Married-civ-spouse|     Adm-clerical|          Wife|             White|Female|         0.0|         0.0|          35.0| United-States| <=50K|\n",
      "|20.0|         Private| 34310.0|Some-college|       Never-married|            Sales|     Own-child|             White|  Male|         0.0|         0.0|          20.0| United-States| <=50K|\n",
      "|49.0|         Private| 81973.0|Some-college|  Married-civ-spouse|     Craft-repair|       Husband|Asian-Pac-Islander|  Male|         0.0|         0.0|          40.0| United-States|  >50K|\n",
      "|61.0|    Self-emp-inc| 66614.0|     HS-grad|  Married-civ-spouse|     Craft-repair|       Husband|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|27.0|         Private|232782.0|Some-college|       Never-married|            Sales|     Own-child|             White|Female|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|19.0|         Private|316868.0|Some-college|       Never-married|    Other-service|     Own-child|             White|  Male|         0.0|         0.0|          30.0|        Mexico| <=50K|\n",
      "|45.0|         Private|196584.0|   Assoc-voc|       Never-married|   Prof-specialty| Not-in-family|             White|Female|         0.0|      1564.0|          40.0| United-States|  >50K|\n",
      "|70.0|         Private|105376.0|Some-college|       Never-married|     Tech-support|Other-relative|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|31.0|         Private|185814.0|     HS-grad|       Never-married| Transport-moving|     Unmarried|             Black|Female|         0.0|         0.0|          30.0| United-States| <=50K|\n",
      "|22.0|         Private|175374.0|Some-college|  Married-civ-spouse|    Other-service|       Husband|             White|  Male|         0.0|         0.0|          24.0| United-States| <=50K|\n",
      "|36.0|         Private|108293.0|     HS-grad|             Widowed|    Other-service|     Unmarried|             White|Female|         0.0|         0.0|          24.0| United-States| <=50K|\n",
      "|64.0|         Private|181232.0|        11th|  Married-civ-spouse|     Craft-repair|       Husband|             White|  Male|         0.0|      2179.0|          40.0| United-States| <=50K|\n",
      "|43.0|               ?|174662.0|Some-college|            Divorced|                ?| Not-in-family|             White|Female|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|47.0|       Local-gov|186009.0|Some-college|            Divorced|     Adm-clerical|     Unmarried|             White|Female|         0.0|         0.0|          38.0|        Mexico| <=50K|\n",
      "|34.0|         Private|198183.0|     HS-grad|       Never-married|     Adm-clerical| Not-in-family|             White|Female|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|33.0|         Private|163003.0|   Bachelors|       Never-married|  Exec-managerial|Other-relative|Asian-Pac-Islander|Female|         0.0|         0.0|          40.0|   Philippines| <=50K|\n",
      "|21.0|         Private|296158.0|     HS-grad|       Never-married|     Craft-repair|     Own-child|             White|  Male|         0.0|         0.0|          35.0| United-States| <=50K|\n",
      "|52.0|               ?|252903.0|     HS-grad|            Divorced|                ?| Not-in-family|             White|  Male|         0.0|         0.0|          45.0| United-States|  >50K|\n",
      "|48.0|         Private|187715.0|     HS-grad|  Married-civ-spouse|     Craft-repair|       Husband|             White|  Male|         0.0|         0.0|          46.0| United-States| <=50K|\n",
      "|23.0|         Private|214542.0|   Bachelors|       Never-married|Handlers-cleaners| Not-in-family|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|71.0|Self-emp-not-inc|494223.0|Some-college|           Separated|            Sales|     Unmarried|             Black|  Male|         0.0|      1816.0|           2.0| United-States| <=50K|\n",
      "|29.0|         Private|191535.0|     HS-grad|            Divorced|     Craft-repair| Not-in-family|             White|  Male|         0.0|         0.0|          60.0| United-States| <=50K|\n",
      "|42.0|         Private|228456.0|   Bachelors|           Separated|    Other-service|Other-relative|             Black|  Male|         0.0|         0.0|          50.0| United-States| <=50K|\n",
      "|68.0|               ?| 38317.0|     1st-4th|            Divorced|                ?| Not-in-family|             White|Female|         0.0|         0.0|          20.0| United-States| <=50K|\n",
      "|25.0|         Private|252752.0|     HS-grad|       Never-married|    Other-service|     Unmarried|             White|Female|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|44.0|    Self-emp-inc| 78374.0|     Masters|            Divorced|  Exec-managerial|     Unmarried|Asian-Pac-Islander|Female|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|28.0|         Private| 88419.0|     HS-grad|       Never-married|  Exec-managerial| Not-in-family|Asian-Pac-Islander|Female|         0.0|         0.0|          40.0|       England| <=50K|\n",
      "|45.0|Self-emp-not-inc|201080.0|     Masters|  Married-civ-spouse|            Sales|       Husband|             White|  Male|         0.0|         0.0|          40.0| United-States|  >50K|\n",
      "|36.0|         Private|207157.0|Some-college|            Divorced|    Other-service|     Unmarried|             White|Female|         0.0|         0.0|          40.0|        Mexico| <=50K|\n",
      "|39.0|     Federal-gov|235485.0|  Assoc-acdm|       Never-married|  Exec-managerial| Not-in-family|             White|  Male|         0.0|         0.0|          42.0| United-States| <=50K|\n",
      "|46.0|       State-gov|102628.0|     Masters|             Widowed|  Protective-serv|     Unmarried|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|18.0|         Private| 25828.0|        11th|       Never-married|Handlers-cleaners|     Own-child|             White|  Male|         0.0|         0.0|          16.0| United-States| <=50K|\n",
      "|66.0|       Local-gov| 54826.0|   Assoc-voc|             Widowed|   Prof-specialty| Not-in-family|             White|Female|         0.0|         0.0|          20.0| United-States| <=50K|\n",
      "|27.0|         Private|124953.0|     HS-grad|       Never-married|    Other-service| Not-in-family|             White|  Male|         0.0|      1980.0|          40.0| United-States| <=50K|\n",
      "|28.0|       State-gov|175325.0|     HS-grad|  Married-civ-spouse|  Protective-serv|       Husband|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|51.0|         Private| 96062.0|Some-college|  Married-civ-spouse|            Sales|       Husband|             White|  Male|         0.0|      1977.0|          40.0| United-States|  >50K|\n",
      "|27.0|         Private|428030.0|   Bachelors|       Never-married|     Craft-repair| Not-in-family|             White|  Male|         0.0|         0.0|          50.0| United-States| <=50K|\n",
      "|28.0|       State-gov|149624.0|   Bachelors|  Married-civ-spouse|   Prof-specialty|       Husband|             White|  Male|         0.0|         0.0|          40.0| United-States|  >50K|\n",
      "|27.0|         Private|253814.0|     HS-grad|Married-spouse-ab...|            Sales|     Unmarried|             White|Female|         0.0|         0.0|          25.0| United-States| <=50K|\n",
      "|21.0|         Private|312956.0|     HS-grad|       Never-married|     Craft-repair|     Own-child|             Black|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|34.0|         Private|483777.0|     HS-grad|       Never-married|Handlers-cleaners| Not-in-family|             Black|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|18.0|         Private|183930.0|     HS-grad|       Never-married|    Other-service|     Own-child|             White|  Male|         0.0|         0.0|          12.0| United-States| <=50K|\n",
      "|33.0|         Private| 37274.0|   Bachelors|  Married-civ-spouse|   Prof-specialty|       Husband|             White|  Male|         0.0|         0.0|          65.0| United-States| <=50K|\n",
      "|44.0|       Local-gov|181344.0|Some-college|  Married-civ-spouse|  Exec-managerial|       Husband|             Black|  Male|         0.0|         0.0|          38.0| United-States|  >50K|\n",
      "|43.0|         Private|114580.0|Some-college|            Divorced|     Adm-clerical| Not-in-family|             White|Female|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|30.0|         Private|633742.0|Some-college|       Never-married|     Craft-repair| Not-in-family|             Black|  Male|         0.0|         0.0|          45.0| United-States| <=50K|\n",
      "|40.0|         Private|286370.0|     7th-8th|  Married-civ-spouse|Machine-op-inspct|       Husband|             White|  Male|         0.0|         0.0|          40.0|        Mexico|  >50K|\n",
      "|37.0|     Federal-gov| 29054.0|Some-college|  Married-civ-spouse|     Adm-clerical|       Husband|             White|  Male|         0.0|         0.0|          42.0| United-States|  >50K|\n",
      "|34.0|         Private|304030.0|     HS-grad|  Married-civ-spouse|     Adm-clerical|       Husband|             Black|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|41.0|Self-emp-not-inc|143129.0|   Bachelors|            Divorced|  Exec-managerial| Not-in-family|             White|Female|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|53.0|               ?|135105.0|   Bachelors|            Divorced|                ?| Not-in-family|             White|Female|         0.0|         0.0|          50.0| United-States| <=50K|\n",
      "|31.0|         Private| 99928.0|     Masters|  Married-civ-spouse|   Prof-specialty|          Wife|             White|Female|         0.0|         0.0|          50.0| United-States| <=50K|\n",
      "|58.0|       State-gov|109567.0|   Doctorate|  Married-civ-spouse|   Prof-specialty|       Husband|             White|  Male|         0.0|         0.0|           1.0| United-States|  >50K|\n",
      "|38.0|         Private|155222.0|Some-college|            Divorced|Machine-op-inspct| Not-in-family|             Black|Female|         0.0|         0.0|          28.0| United-States| <=50K|\n",
      "|24.0|         Private|159567.0|Some-college|  Married-civ-spouse|Machine-op-inspct|       Husband|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|41.0|       Local-gov|523910.0|   Bachelors|  Married-civ-spouse|     Craft-repair|       Husband|             Black|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|47.0|         Private|120939.0|Some-college|  Married-civ-spouse|     Tech-support|       Husband|             White|  Male|         0.0|         0.0|          45.0| United-States| <=50K|\n",
      "|41.0|     Federal-gov|130760.0|   Bachelors|  Married-civ-spouse|     Tech-support|       Husband|             White|  Male|         0.0|         0.0|          24.0| United-States| <=50K|\n",
      "|23.0|         Private|197387.0|     5th-6th|  Married-civ-spouse| Transport-moving|Other-relative|             White|  Male|         0.0|         0.0|          40.0|        Mexico| <=50K|\n",
      "|36.0|         Private| 99374.0|Some-college|            Divorced|     Craft-repair| Not-in-family|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "|40.0|     Federal-gov| 56795.0|     Masters|       Never-married|  Exec-managerial| Not-in-family|             White|Female|     14084.0|         0.0|          55.0| United-States|  >50K|\n",
      "|35.0|         Private|138992.0|     Masters|  Married-civ-spouse|   Prof-specialty|Other-relative|             White|  Male|      7298.0|         0.0|          40.0| United-States|  >50K|\n",
      "|24.0|Self-emp-not-inc| 32921.0|     HS-grad|       Never-married|            Sales| Not-in-family|             White|  Male|         0.0|         0.0|          40.0| United-States| <=50K|\n",
      "+----+----------------+--------+------------+--------------------+-----------------+--------------+------------------+------+------------+------------+--------------+--------------+------+\n",
      "only showing top 200 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfraw.show(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, some cells contain `?` as their values. Those are missing values. They appear only in these columns: `workclass`, `occupation` and `native_country`. Display the most frequent values from the `workclass` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|       workclass|count|\n",
      "+----------------+-----+\n",
      "|         Private|33906|\n",
      "|Self-emp-not-inc| 3862|\n",
      "|       Local-gov| 3136|\n",
      "|               ?| 2799|\n",
      "|       State-gov| 1981|\n",
      "|    Self-emp-inc| 1695|\n",
      "|     Federal-gov| 1432|\n",
      "|     Without-pay|   21|\n",
      "|    Never-worked|   10|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "dfraw.groupBy(dfraw.workclass).count().orderBy(F.col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for the other two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|       occupation|count|\n",
      "+-----------------+-----+\n",
      "|   Prof-specialty| 6172|\n",
      "|     Craft-repair| 6112|\n",
      "|  Exec-managerial| 6086|\n",
      "|     Adm-clerical| 5611|\n",
      "|            Sales| 5504|\n",
      "|    Other-service| 4923|\n",
      "|Machine-op-inspct| 3022|\n",
      "|                ?| 2809|\n",
      "| Transport-moving| 2355|\n",
      "|Handlers-cleaners| 2072|\n",
      "|  Farming-fishing| 1490|\n",
      "|     Tech-support| 1446|\n",
      "|  Protective-serv|  983|\n",
      "|  Priv-house-serv|  242|\n",
      "|     Armed-Forces|   15|\n",
      "+-----------------+-----+\n",
      "\n",
      "+------------------+-----+\n",
      "|    native_country|count|\n",
      "+------------------+-----+\n",
      "|     United-States|43832|\n",
      "|            Mexico|  951|\n",
      "|                 ?|  857|\n",
      "|       Philippines|  295|\n",
      "|           Germany|  206|\n",
      "|       Puerto-Rico|  184|\n",
      "|            Canada|  182|\n",
      "|       El-Salvador|  155|\n",
      "|             India|  151|\n",
      "|              Cuba|  138|\n",
      "|           England|  127|\n",
      "|             China|  122|\n",
      "|             South|  115|\n",
      "|           Jamaica|  106|\n",
      "|             Italy|  105|\n",
      "|Dominican-Republic|  103|\n",
      "|             Japan|   92|\n",
      "|         Guatemala|   88|\n",
      "|            Poland|   87|\n",
      "|           Vietnam|   86|\n",
      "+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfraw.groupBy(dfraw.occupation).count().orderBy(F.col('count').desc()).show()\n",
    "dfraw.groupBy(dfraw.native_country).count().orderBy(F.col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the `DataFrame`'s built-in `na.replace` method to change all missing values into the most frequent value of the corresponding column (`Private` for `workclass`, `Prof-specialty` for `occupation` and `United-States` for `native_country`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfrawrp = dfraw.na.replace({\"?\": \"Private\"}, subset=[\"workclass\"])\n",
    "dfrawrpl = dfrawrp.na.replace({\"?\": \"Prof-specialty\"}, subset=[\"occupation\"])\n",
    "dfrawnona = dfrawrpl.na.replace({\"?\": \"United-States\"}, subset=[\"native_country\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the new DataFrame to see if the changes have been made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|       workclass|count|\n",
      "+----------------+-----+\n",
      "|         Private|36705|\n",
      "|Self-emp-not-inc| 3862|\n",
      "|       Local-gov| 3136|\n",
      "|       State-gov| 1981|\n",
      "|    Self-emp-inc| 1695|\n",
      "+----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-----+\n",
      "|       occupation|count|\n",
      "+-----------------+-----+\n",
      "|   Prof-specialty| 8981|\n",
      "|     Craft-repair| 6112|\n",
      "|  Exec-managerial| 6086|\n",
      "|     Adm-clerical| 5611|\n",
      "|            Sales| 5504|\n",
      "|    Other-service| 4923|\n",
      "|Machine-op-inspct| 3022|\n",
      "| Transport-moving| 2355|\n",
      "|Handlers-cleaners| 2072|\n",
      "+-----------------+-----+\n",
      "only showing top 9 rows\n",
      "\n",
      "+--------------+-----+\n",
      "|native_country|count|\n",
      "+--------------+-----+\n",
      "| United-States|44689|\n",
      "|        Mexico|  951|\n",
      "|   Philippines|  295|\n",
      "|       Germany|  206|\n",
      "+--------------+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfrawnona.groupBy(dfrawnona.workclass).count().orderBy(F.col('count').desc()).show(5)\n",
    "dfrawnona.groupBy(dfrawnona.occupation).count().orderBy(F.col('count').desc()).show(9)\n",
    "dfrawnona.groupBy(dfrawnona.native_country).count().orderBy(F.col('count').desc()).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't have any more missing values, but the other categorical values cannot be used by machine learning algorithms as strings. They have to be converted to numerical values, but a naive solution of simply enumerating them wouldn't work because that would imply existence of a ranking scheme, while no such scheme exists in the real world. What you can do instead is to on-hot-encode those values into several columns.\n",
    "\n",
    "For that you need Spark's `StringIndexer`, `OneHotEncoder` and `VectorAssembler` classes.\n",
    "\n",
    "Write a method which takes a DataFrame and a list of columns and replaces each column from the list with its string-indexed version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "def indexStringColumns(df, cols):\n",
    "    newdf = df\n",
    "    for col in cols:\n",
    "        si = StringIndexer(inputCol=col, outputCol=col+\"-num\")\n",
    "        sm = si.fit(newdf)\n",
    "        newdf = sm.transform(newdf).drop(col)\n",
    "        newdf = newdf.withColumnRenamed(col+\"-num\", col)\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use it now on your DataFrame with no missing values and string-index the following columns: `workclass`, `education`, `marital_status`, `occupation`, `relationship`, `race`, `sex`, `native_country`, `income`. Inspect the output to see what the function has done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+------------+------------+--------------+---------+---------+--------------+----------+------------+----+---+--------------+------+\n",
      "| age|  fnlwgt|capital_gain|capital_loss|hours_per_week|workclass|education|marital_status|occupation|relationship|race|sex|native_country|income|\n",
      "+----+--------+------------+------------+--------------+---------+---------+--------------+----------+------------+----+---+--------------+------+\n",
      "|39.0| 77516.0|      2174.0|         0.0|          40.0|      3.0|      2.0|           1.0|       3.0|         1.0| 0.0|0.0|           0.0|   0.0|\n",
      "|50.0| 83311.0|         0.0|         0.0|          13.0|      1.0|      2.0|           0.0|       2.0|         0.0| 0.0|0.0|           0.0|   0.0|\n",
      "|38.0|215646.0|         0.0|         0.0|          40.0|      0.0|      0.0|           2.0|       8.0|         1.0| 0.0|0.0|           0.0|   0.0|\n",
      "|53.0|234721.0|         0.0|         0.0|          40.0|      0.0|      5.0|           0.0|       8.0|         0.0| 1.0|0.0|           0.0|   0.0|\n",
      "|28.0|338409.0|         0.0|         0.0|          40.0|      0.0|      2.0|           0.0|       0.0|         4.0| 1.0|1.0|           8.0|   0.0|\n",
      "|37.0|284582.0|         0.0|         0.0|          40.0|      0.0|      3.0|           0.0|       2.0|         4.0| 0.0|1.0|           0.0|   0.0|\n",
      "|49.0|160187.0|         0.0|         0.0|          16.0|      0.0|     10.0|           5.0|       5.0|         1.0| 1.0|1.0|          12.0|   0.0|\n",
      "|52.0|209642.0|         0.0|         0.0|          45.0|      1.0|      0.0|           0.0|       2.0|         0.0| 0.0|0.0|           0.0|   1.0|\n",
      "|31.0| 45781.0|     14084.0|         0.0|          50.0|      0.0|      3.0|           1.0|       0.0|         1.0| 0.0|1.0|           0.0|   1.0|\n",
      "|42.0|159449.0|      5178.0|         0.0|          40.0|      0.0|      2.0|           0.0|       2.0|         0.0| 0.0|0.0|           0.0|   1.0|\n",
      "|37.0|280464.0|         0.0|         0.0|          80.0|      0.0|      1.0|           0.0|       2.0|         0.0| 1.0|0.0|           0.0|   1.0|\n",
      "|30.0|141297.0|         0.0|         0.0|          40.0|      3.0|      2.0|           0.0|       0.0|         0.0| 2.0|0.0|           7.0|   1.0|\n",
      "|23.0|122272.0|         0.0|         0.0|          30.0|      0.0|      2.0|           1.0|       3.0|         2.0| 0.0|1.0|           0.0|   0.0|\n",
      "|32.0|205019.0|         0.0|         0.0|          50.0|      0.0|      6.0|           1.0|       4.0|         1.0| 1.0|0.0|           0.0|   0.0|\n",
      "|40.0|121772.0|         0.0|         0.0|          40.0|      0.0|      4.0|           0.0|       1.0|         0.0| 2.0|0.0|           0.0|   1.0|\n",
      "|34.0|245487.0|         0.0|         0.0|          45.0|      0.0|      8.0|           0.0|       7.0|         0.0| 3.0|0.0|           1.0|   0.0|\n",
      "|25.0|176756.0|         0.0|         0.0|          35.0|      1.0|      0.0|           1.0|       9.0|         2.0| 0.0|0.0|           0.0|   0.0|\n",
      "|32.0|186824.0|         0.0|         0.0|          40.0|      0.0|      0.0|           1.0|       6.0|         3.0| 0.0|0.0|           0.0|   0.0|\n",
      "|38.0| 28887.0|         0.0|         0.0|          50.0|      0.0|      5.0|           0.0|       4.0|         0.0| 0.0|0.0|           0.0|   0.0|\n",
      "|43.0|292175.0|         0.0|         0.0|          45.0|      1.0|      3.0|           2.0|       2.0|         3.0| 0.0|1.0|           0.0|   1.0|\n",
      "+----+--------+------------+------------+--------------+---------+---------+--------------+----------+------------+----+---+--------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfnumeric = indexStringColumns(dfrawnona, [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \n",
    "                                           \"race\", \"sex\", \"native_country\", \"income\"])\n",
    "dfnumeric.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the string values have been replaced with numerical values. The following function will one-hot-encode those values into their separate columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncodeColumns(df, cols):\n",
    "    from pyspark.ml.feature import OneHotEncoder\n",
    "    newdf = df\n",
    "    for c in cols:\n",
    "        onehotenc = OneHotEncoder(inputCol=c, outputCol=c+\"-onehot\", dropLast=False)\n",
    "        newdf = onehotenc.transform(newdf).drop(c)\n",
    "        newdf = newdf.withColumnRenamed(c+\"-onehot\", c)\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use it now to one-hot-encode the string-indexed columns. Inspect the resulting DataFrame to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+------------+------------+--------------+---+------+-------------+---------------+--------------+--------------+-------------+-------------+---------------+\n",
      "| age|  fnlwgt|capital_gain|capital_loss|hours_per_week|sex|income|    workclass|      education|marital_status|    occupation| relationship|         race| native_country|\n",
      "+----+--------+------------+------------+--------------+---+------+-------------+---------------+--------------+--------------+-------------+-------------+---------------+\n",
      "|39.0| 77516.0|      2174.0|         0.0|          40.0|0.0|   0.0|(8,[3],[1.0])| (16,[2],[1.0])| (7,[1],[1.0])|(14,[3],[1.0])|(6,[1],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|50.0| 83311.0|         0.0|         0.0|          13.0|0.0|   0.0|(8,[1],[1.0])| (16,[2],[1.0])| (7,[0],[1.0])|(14,[2],[1.0])|(6,[0],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|38.0|215646.0|         0.0|         0.0|          40.0|0.0|   0.0|(8,[0],[1.0])| (16,[0],[1.0])| (7,[2],[1.0])|(14,[8],[1.0])|(6,[1],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|53.0|234721.0|         0.0|         0.0|          40.0|0.0|   0.0|(8,[0],[1.0])| (16,[5],[1.0])| (7,[0],[1.0])|(14,[8],[1.0])|(6,[0],[1.0])|(5,[1],[1.0])| (41,[0],[1.0])|\n",
      "|28.0|338409.0|         0.0|         0.0|          40.0|1.0|   0.0|(8,[0],[1.0])| (16,[2],[1.0])| (7,[0],[1.0])|(14,[0],[1.0])|(6,[4],[1.0])|(5,[1],[1.0])| (41,[8],[1.0])|\n",
      "|37.0|284582.0|         0.0|         0.0|          40.0|1.0|   0.0|(8,[0],[1.0])| (16,[3],[1.0])| (7,[0],[1.0])|(14,[2],[1.0])|(6,[4],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|49.0|160187.0|         0.0|         0.0|          16.0|1.0|   0.0|(8,[0],[1.0])|(16,[10],[1.0])| (7,[5],[1.0])|(14,[5],[1.0])|(6,[1],[1.0])|(5,[1],[1.0])|(41,[12],[1.0])|\n",
      "|52.0|209642.0|         0.0|         0.0|          45.0|0.0|   1.0|(8,[1],[1.0])| (16,[0],[1.0])| (7,[0],[1.0])|(14,[2],[1.0])|(6,[0],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|31.0| 45781.0|     14084.0|         0.0|          50.0|1.0|   1.0|(8,[0],[1.0])| (16,[3],[1.0])| (7,[1],[1.0])|(14,[0],[1.0])|(6,[1],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|42.0|159449.0|      5178.0|         0.0|          40.0|0.0|   1.0|(8,[0],[1.0])| (16,[2],[1.0])| (7,[0],[1.0])|(14,[2],[1.0])|(6,[0],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|37.0|280464.0|         0.0|         0.0|          80.0|0.0|   1.0|(8,[0],[1.0])| (16,[1],[1.0])| (7,[0],[1.0])|(14,[2],[1.0])|(6,[0],[1.0])|(5,[1],[1.0])| (41,[0],[1.0])|\n",
      "|30.0|141297.0|         0.0|         0.0|          40.0|0.0|   1.0|(8,[3],[1.0])| (16,[2],[1.0])| (7,[0],[1.0])|(14,[0],[1.0])|(6,[0],[1.0])|(5,[2],[1.0])| (41,[7],[1.0])|\n",
      "|23.0|122272.0|         0.0|         0.0|          30.0|1.0|   0.0|(8,[0],[1.0])| (16,[2],[1.0])| (7,[1],[1.0])|(14,[3],[1.0])|(6,[2],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|32.0|205019.0|         0.0|         0.0|          50.0|0.0|   0.0|(8,[0],[1.0])| (16,[6],[1.0])| (7,[1],[1.0])|(14,[4],[1.0])|(6,[1],[1.0])|(5,[1],[1.0])| (41,[0],[1.0])|\n",
      "|40.0|121772.0|         0.0|         0.0|          40.0|0.0|   1.0|(8,[0],[1.0])| (16,[4],[1.0])| (7,[0],[1.0])|(14,[1],[1.0])|(6,[0],[1.0])|(5,[2],[1.0])| (41,[0],[1.0])|\n",
      "|34.0|245487.0|         0.0|         0.0|          45.0|0.0|   0.0|(8,[0],[1.0])| (16,[8],[1.0])| (7,[0],[1.0])|(14,[7],[1.0])|(6,[0],[1.0])|(5,[3],[1.0])| (41,[1],[1.0])|\n",
      "|25.0|176756.0|         0.0|         0.0|          35.0|0.0|   0.0|(8,[1],[1.0])| (16,[0],[1.0])| (7,[1],[1.0])|(14,[9],[1.0])|(6,[2],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|32.0|186824.0|         0.0|         0.0|          40.0|0.0|   0.0|(8,[0],[1.0])| (16,[0],[1.0])| (7,[1],[1.0])|(14,[6],[1.0])|(6,[3],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|38.0| 28887.0|         0.0|         0.0|          50.0|0.0|   0.0|(8,[0],[1.0])| (16,[5],[1.0])| (7,[0],[1.0])|(14,[4],[1.0])|(6,[0],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "|43.0|292175.0|         0.0|         0.0|          45.0|1.0|   1.0|(8,[1],[1.0])| (16,[3],[1.0])| (7,[2],[1.0])|(14,[2],[1.0])|(6,[3],[1.0])|(5,[0],[1.0])| (41,[0],[1.0])|\n",
      "+----+--------+------------+------------+--------------+---+------+-------------+---------------+--------------+--------------+-------------+-------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfhot = oneHotEncodeColumns(dfnumeric, [\"workclass\", \"education\", \"marital_status\", \"occupation\", \n",
    "                                        \"relationship\", \"race\", \"native_country\"])\n",
    "dfhot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, each one-hot-encoded column now contains arrays. The next step is to use `VectorAssembler` to merge all the columns into a single column called `features`. \n",
    "\n",
    "Construct a new instance of `VectorAssembler`, set its output columns to be `features` and set its input columns to be all columns except `income`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "cols = dfhot.columns\n",
    "cols.remove(\"income\")\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use this `VectorAssembler` instance to transform the DataFrame with one-hot-encoded columns. Then preserve only the `features` and `income` columns (use `select`). Finally, rename the *income* column to *label*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpoints = va.transform(dfhot).select(\"features\", \"income\").withColumnRenamed(\"income\", \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the resulting DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(103,[0,1,2,4,9,1...|  0.0|\n",
      "|(103,[0,1,4,7,16,...|  0.0|\n",
      "|(103,[0,1,4,6,14,...|  0.0|\n",
      "|(103,[0,1,4,6,19,...|  0.0|\n",
      "|(103,[0,1,4,5,6,1...|  0.0|\n",
      "|(103,[0,1,4,5,6,1...|  0.0|\n",
      "|(103,[0,1,4,5,6,2...|  0.0|\n",
      "|(103,[0,1,4,7,14,...|  1.0|\n",
      "|(103,[0,1,2,4,5,6...|  1.0|\n",
      "|(103,[0,1,2,4,6,1...|  1.0|\n",
      "|(103,[0,1,4,6,15,...|  1.0|\n",
      "|(103,[0,1,4,9,16,...|  1.0|\n",
      "|(103,[0,1,4,5,6,1...|  0.0|\n",
      "|(103,[0,1,4,6,20,...|  0.0|\n",
      "|(103,[0,1,4,6,18,...|  1.0|\n",
      "|(103,[0,1,4,6,22,...|  0.0|\n",
      "|(103,[0,1,4,7,14,...|  0.0|\n",
      "|(103,[0,1,4,6,14,...|  0.0|\n",
      "|(103,[0,1,4,6,19,...|  0.0|\n",
      "|(103,[0,1,4,5,7,1...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lpoints.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                         |label|\n",
      "+-------------------------------------------------------------------------------------------------+-----+\n",
      "|(103,[0,1,2,4,9,16,31,40,52,57,62],[39.0,77516.0,2174.0,40.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])       |0.0  |\n",
      "|(103,[0,1,4,7,16,30,39,51,57,62],[50.0,83311.0,13.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                |0.0  |\n",
      "|(103,[0,1,4,6,14,32,45,52,57,62],[38.0,215646.0,40.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])               |0.0  |\n",
      "|(103,[0,1,4,6,19,30,45,51,58,62],[53.0,234721.0,40.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])               |0.0  |\n",
      "|(103,[0,1,4,5,6,16,30,37,55,58,70],[28.0,338409.0,40.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])         |0.0  |\n",
      "|(103,[0,1,4,5,6,17,30,39,55,57,62],[37.0,284582.0,40.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])         |0.0  |\n",
      "|(103,[0,1,4,5,6,24,35,42,52,58,74],[49.0,160187.0,16.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])         |0.0  |\n",
      "|(103,[0,1,4,7,14,30,39,51,57,62],[52.0,209642.0,45.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])               |1.0  |\n",
      "|(103,[0,1,2,4,5,6,17,31,37,52,57,62],[31.0,45781.0,14084.0,50.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|1.0  |\n",
      "|(103,[0,1,2,4,6,16,30,39,51,57,62],[42.0,159449.0,5178.0,40.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])      |1.0  |\n",
      "|(103,[0,1,4,6,15,30,39,51,58,62],[37.0,280464.0,80.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])               |1.0  |\n",
      "|(103,[0,1,4,9,16,30,37,51,59,69],[30.0,141297.0,40.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])               |1.0  |\n",
      "|(103,[0,1,4,5,6,16,31,40,53,57,62],[23.0,122272.0,30.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])         |0.0  |\n",
      "|(103,[0,1,4,6,20,31,41,52,58,62],[32.0,205019.0,50.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])               |0.0  |\n",
      "|(103,[0,1,4,6,18,30,38,51,59,62],[40.0,121772.0,40.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])               |1.0  |\n",
      "|(103,[0,1,4,6,22,30,44,51,60,63],[34.0,245487.0,45.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])               |0.0  |\n",
      "|(103,[0,1,4,7,14,31,46,53,57,62],[25.0,176756.0,35.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])               |0.0  |\n",
      "|(103,[0,1,4,6,14,31,43,54,57,62],[32.0,186824.0,40.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])               |0.0  |\n",
      "|(103,[0,1,4,6,19,30,41,51,57,62],[38.0,28887.0,50.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                |0.0  |\n",
      "|(103,[0,1,4,5,7,17,32,39,54,57,62],[43.0,292175.0,45.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])         |1.0  |\n",
      "+-------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lpoints.show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how pipelines work, let's repeat the same procedure, but using the `pyspark.ml.Pipeline` class. First, create a DataFrame with the \"income\" column renamed to \"label\", using the DataFrame without missing data (the one you created above). (Just call `withColumnRenamed`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipdf = dfrawnona.withColumnRenamed('income', 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now execute the following cell which defines two lists with names of columns to string-index and the names of the remaining columns (without \"label\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_encode = [\"workclass\", \"education\", \"marital_status\", \"sex\", \"occupation\", \"relationship\", \"race\", \"native_country\"]\n",
    "other_columns = ['age', 'fnlwgt', 'capital_gain', 'capital_loss', 'hours_per_week']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step create a list of `StringIndexer` and a list of `OneHotEncoder` objects for each column from `columns_to_encode` list. `OneHotEncoder` objects should reference columns created by `StringIndexer` objects and produce columns with original names sufixed with \"-onehot\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "def createIndexers(cols):\n",
    "    return [StringIndexer(inputCol=col, outputCol=col+\"-num\") for col in cols]\n",
    "\n",
    "def createOneHots(cols):\n",
    "    return [OneHotEncoder(inputCol=col+\"-num\", outputCol=col+\"-onehot\", dropLast=False) for col in cols]\n",
    "\n",
    "indexers = createIndexers(columns_to_encode)\n",
    "onehots = createOneHots(columns_to_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the list of column names which will be assembled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_assemble = [c+'-onehot' for c in columns_to_encode] + other_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a `VectorAssembler` which creates `features` columns from columns in `columns_to_assemble`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "va = VectorAssembler(outputCol=\"features\", inputCols=columns_to_assemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create a `Pipeline` object with stages comprised of all `StringIndexer`, all `OneHotEncoder` objects and the `VectorAssembler` object (in that order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pip = Pipeline(stages=indexers+onehots+[va])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can `fit` the pipeline on the input DataFrame and then use the resulting model to `transform` the same dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipmodel = pip.fit(pipdf)\n",
    "pippred = pipmodel.transform(pipdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the resulting DataFrame to see if everything is OK and then create the final DataFrame containing only `features` and `label` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+--------+------------+--------------------+-----------------+-------------+------------------+------+------------+------------+--------------+--------------+-----+-------------+-------------+------------------+-------+--------------+----------------+--------+------------------+----------------+----------------+---------------------+-------------+-----------------+-------------------+-------------+---------------------+--------------------+\n",
      "| age|       workclass|  fnlwgt|   education|      marital_status|       occupation| relationship|              race|   sex|capital_gain|capital_loss|hours_per_week|native_country|label|workclass-num|education-num|marital_status-num|sex-num|occupation-num|relationship-num|race-num|native_country-num|workclass-onehot|education-onehot|marital_status-onehot|   sex-onehot|occupation-onehot|relationship-onehot|  race-onehot|native_country-onehot|            features|\n",
      "+----+----------------+--------+------------+--------------------+-----------------+-------------+------------------+------+------------+------------+--------------+--------------+-----+-------------+-------------+------------------+-------+--------------+----------------+--------+------------------+----------------+----------------+---------------------+-------------+-----------------+-------------------+-------------+---------------------+--------------------+\n",
      "|39.0|       State-gov| 77516.0|   Bachelors|       Never-married|     Adm-clerical|Not-in-family|             White|  Male|      2174.0|         0.0|          40.0| United-States|<=50K|          3.0|          2.0|               1.0|    0.0|           3.0|             1.0|     0.0|               0.0|   (8,[3],[1.0])|  (16,[2],[1.0])|        (7,[1],[1.0])|(2,[0],[1.0])|   (14,[3],[1.0])|      (6,[1],[1.0])|(5,[0],[1.0])|       (41,[0],[1.0])|(104,[3,10,25,31,...|\n",
      "|50.0|Self-emp-not-inc| 83311.0|   Bachelors|  Married-civ-spouse|  Exec-managerial|      Husband|             White|  Male|         0.0|         0.0|          13.0| United-States|<=50K|          1.0|          2.0|               0.0|    0.0|           2.0|             0.0|     0.0|               0.0|   (8,[1],[1.0])|  (16,[2],[1.0])|        (7,[0],[1.0])|(2,[0],[1.0])|   (14,[2],[1.0])|      (6,[0],[1.0])|(5,[0],[1.0])|       (41,[0],[1.0])|(104,[1,10,24,31,...|\n",
      "|38.0|         Private|215646.0|     HS-grad|            Divorced|Handlers-cleaners|Not-in-family|             White|  Male|         0.0|         0.0|          40.0| United-States|<=50K|          0.0|          0.0|               2.0|    0.0|           8.0|             1.0|     0.0|               0.0|   (8,[0],[1.0])|  (16,[0],[1.0])|        (7,[2],[1.0])|(2,[0],[1.0])|   (14,[8],[1.0])|      (6,[1],[1.0])|(5,[0],[1.0])|       (41,[0],[1.0])|(104,[0,8,26,31,4...|\n",
      "|53.0|         Private|234721.0|        11th|  Married-civ-spouse|Handlers-cleaners|      Husband|             Black|  Male|         0.0|         0.0|          40.0| United-States|<=50K|          0.0|          5.0|               0.0|    0.0|           8.0|             0.0|     1.0|               0.0|   (8,[0],[1.0])|  (16,[5],[1.0])|        (7,[0],[1.0])|(2,[0],[1.0])|   (14,[8],[1.0])|      (6,[0],[1.0])|(5,[1],[1.0])|       (41,[0],[1.0])|(104,[0,13,24,31,...|\n",
      "|28.0|         Private|338409.0|   Bachelors|  Married-civ-spouse|   Prof-specialty|         Wife|             Black|Female|         0.0|         0.0|          40.0|          Cuba|<=50K|          0.0|          2.0|               0.0|    1.0|           0.0|             4.0|     1.0|               8.0|   (8,[0],[1.0])|  (16,[2],[1.0])|        (7,[0],[1.0])|(2,[1],[1.0])|   (14,[0],[1.0])|      (6,[4],[1.0])|(5,[1],[1.0])|       (41,[8],[1.0])|(104,[0,10,24,32,...|\n",
      "|37.0|         Private|284582.0|     Masters|  Married-civ-spouse|  Exec-managerial|         Wife|             White|Female|         0.0|         0.0|          40.0| United-States|<=50K|          0.0|          3.0|               0.0|    1.0|           2.0|             4.0|     0.0|               0.0|   (8,[0],[1.0])|  (16,[3],[1.0])|        (7,[0],[1.0])|(2,[1],[1.0])|   (14,[2],[1.0])|      (6,[4],[1.0])|(5,[0],[1.0])|       (41,[0],[1.0])|(104,[0,11,24,32,...|\n",
      "|49.0|         Private|160187.0|         9th|Married-spouse-ab...|    Other-service|Not-in-family|             Black|Female|         0.0|         0.0|          16.0|       Jamaica|<=50K|          0.0|         10.0|               5.0|    1.0|           5.0|             1.0|     1.0|              12.0|   (8,[0],[1.0])| (16,[10],[1.0])|        (7,[5],[1.0])|(2,[1],[1.0])|   (14,[5],[1.0])|      (6,[1],[1.0])|(5,[1],[1.0])|      (41,[12],[1.0])|(104,[0,18,29,32,...|\n",
      "|52.0|Self-emp-not-inc|209642.0|     HS-grad|  Married-civ-spouse|  Exec-managerial|      Husband|             White|  Male|         0.0|         0.0|          45.0| United-States| >50K|          1.0|          0.0|               0.0|    0.0|           2.0|             0.0|     0.0|               0.0|   (8,[1],[1.0])|  (16,[0],[1.0])|        (7,[0],[1.0])|(2,[0],[1.0])|   (14,[2],[1.0])|      (6,[0],[1.0])|(5,[0],[1.0])|       (41,[0],[1.0])|(104,[1,8,24,31,3...|\n",
      "|31.0|         Private| 45781.0|     Masters|       Never-married|   Prof-specialty|Not-in-family|             White|Female|     14084.0|         0.0|          50.0| United-States| >50K|          0.0|          3.0|               1.0|    1.0|           0.0|             1.0|     0.0|               0.0|   (8,[0],[1.0])|  (16,[3],[1.0])|        (7,[1],[1.0])|(2,[1],[1.0])|   (14,[0],[1.0])|      (6,[1],[1.0])|(5,[0],[1.0])|       (41,[0],[1.0])|(104,[0,11,25,32,...|\n",
      "|42.0|         Private|159449.0|   Bachelors|  Married-civ-spouse|  Exec-managerial|      Husband|             White|  Male|      5178.0|         0.0|          40.0| United-States| >50K|          0.0|          2.0|               0.0|    0.0|           2.0|             0.0|     0.0|               0.0|   (8,[0],[1.0])|  (16,[2],[1.0])|        (7,[0],[1.0])|(2,[0],[1.0])|   (14,[2],[1.0])|      (6,[0],[1.0])|(5,[0],[1.0])|       (41,[0],[1.0])|(104,[0,10,24,31,...|\n",
      "|37.0|         Private|280464.0|Some-college|  Married-civ-spouse|  Exec-managerial|      Husband|             Black|  Male|         0.0|         0.0|          80.0| United-States| >50K|          0.0|          1.0|               0.0|    0.0|           2.0|             0.0|     1.0|               0.0|   (8,[0],[1.0])|  (16,[1],[1.0])|        (7,[0],[1.0])|(2,[0],[1.0])|   (14,[2],[1.0])|      (6,[0],[1.0])|(5,[1],[1.0])|       (41,[0],[1.0])|(104,[0,9,24,31,3...|\n",
      "|30.0|       State-gov|141297.0|   Bachelors|  Married-civ-spouse|   Prof-specialty|      Husband|Asian-Pac-Islander|  Male|         0.0|         0.0|          40.0|         India| >50K|          3.0|          2.0|               0.0|    0.0|           0.0|             0.0|     2.0|               7.0|   (8,[3],[1.0])|  (16,[2],[1.0])|        (7,[0],[1.0])|(2,[0],[1.0])|   (14,[0],[1.0])|      (6,[0],[1.0])|(5,[2],[1.0])|       (41,[7],[1.0])|(104,[3,10,24,31,...|\n",
      "|23.0|         Private|122272.0|   Bachelors|       Never-married|     Adm-clerical|    Own-child|             White|Female|         0.0|         0.0|          30.0| United-States|<=50K|          0.0|          2.0|               1.0|    1.0|           3.0|             2.0|     0.0|               0.0|   (8,[0],[1.0])|  (16,[2],[1.0])|        (7,[1],[1.0])|(2,[1],[1.0])|   (14,[3],[1.0])|      (6,[2],[1.0])|(5,[0],[1.0])|       (41,[0],[1.0])|(104,[0,10,25,32,...|\n",
      "|32.0|         Private|205019.0|  Assoc-acdm|       Never-married|            Sales|Not-in-family|             Black|  Male|         0.0|         0.0|          50.0| United-States|<=50K|          0.0|          6.0|               1.0|    0.0|           4.0|             1.0|     1.0|               0.0|   (8,[0],[1.0])|  (16,[6],[1.0])|        (7,[1],[1.0])|(2,[0],[1.0])|   (14,[4],[1.0])|      (6,[1],[1.0])|(5,[1],[1.0])|       (41,[0],[1.0])|(104,[0,14,25,31,...|\n",
      "|40.0|         Private|121772.0|   Assoc-voc|  Married-civ-spouse|     Craft-repair|      Husband|Asian-Pac-Islander|  Male|         0.0|         0.0|          40.0| United-States| >50K|          0.0|          4.0|               0.0|    0.0|           1.0|             0.0|     2.0|               0.0|   (8,[0],[1.0])|  (16,[4],[1.0])|        (7,[0],[1.0])|(2,[0],[1.0])|   (14,[1],[1.0])|      (6,[0],[1.0])|(5,[2],[1.0])|       (41,[0],[1.0])|(104,[0,12,24,31,...|\n",
      "|34.0|         Private|245487.0|     7th-8th|  Married-civ-spouse| Transport-moving|      Husband|Amer-Indian-Eskimo|  Male|         0.0|         0.0|          45.0|        Mexico|<=50K|          0.0|          8.0|               0.0|    0.0|           7.0|             0.0|     3.0|               1.0|   (8,[0],[1.0])|  (16,[8],[1.0])|        (7,[0],[1.0])|(2,[0],[1.0])|   (14,[7],[1.0])|      (6,[0],[1.0])|(5,[3],[1.0])|       (41,[1],[1.0])|(104,[0,16,24,31,...|\n",
      "|25.0|Self-emp-not-inc|176756.0|     HS-grad|       Never-married|  Farming-fishing|    Own-child|             White|  Male|         0.0|         0.0|          35.0| United-States|<=50K|          1.0|          0.0|               1.0|    0.0|           9.0|             2.0|     0.0|               0.0|   (8,[1],[1.0])|  (16,[0],[1.0])|        (7,[1],[1.0])|(2,[0],[1.0])|   (14,[9],[1.0])|      (6,[2],[1.0])|(5,[0],[1.0])|       (41,[0],[1.0])|(104,[1,8,25,31,4...|\n",
      "|32.0|         Private|186824.0|     HS-grad|       Never-married|Machine-op-inspct|    Unmarried|             White|  Male|         0.0|         0.0|          40.0| United-States|<=50K|          0.0|          0.0|               1.0|    0.0|           6.0|             3.0|     0.0|               0.0|   (8,[0],[1.0])|  (16,[0],[1.0])|        (7,[1],[1.0])|(2,[0],[1.0])|   (14,[6],[1.0])|      (6,[3],[1.0])|(5,[0],[1.0])|       (41,[0],[1.0])|(104,[0,8,25,31,3...|\n",
      "|38.0|         Private| 28887.0|        11th|  Married-civ-spouse|            Sales|      Husband|             White|  Male|         0.0|         0.0|          50.0| United-States|<=50K|          0.0|          5.0|               0.0|    0.0|           4.0|             0.0|     0.0|               0.0|   (8,[0],[1.0])|  (16,[5],[1.0])|        (7,[0],[1.0])|(2,[0],[1.0])|   (14,[4],[1.0])|      (6,[0],[1.0])|(5,[0],[1.0])|       (41,[0],[1.0])|(104,[0,13,24,31,...|\n",
      "|43.0|Self-emp-not-inc|292175.0|     Masters|            Divorced|  Exec-managerial|    Unmarried|             White|Female|         0.0|         0.0|          45.0| United-States| >50K|          1.0|          3.0|               2.0|    1.0|           2.0|             3.0|     0.0|               0.0|   (8,[1],[1.0])|  (16,[3],[1.0])|        (7,[2],[1.0])|(2,[1],[1.0])|   (14,[2],[1.0])|      (6,[3],[1.0])|(5,[0],[1.0])|       (41,[0],[1.0])|(104,[1,11,26,32,...|\n",
      "+----+----------------+--------+------------+--------------------+-----------------+-------------+------------------+------+------------+------------+--------------+--------------+-----+-------------+-------------+------------------+-------+--------------+----------------+--------+------------------+----------------+----------------+---------------------+-------------+-----------------+-------------------+-------------+---------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pippred.show()\n",
    "final = pippred.select('features', 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have only two columns: *features*, encoded as a sparse vector, and *label*, containing the target value. \n",
    "\n",
    "Split the DataFrame into a training DataFrame containing 80% of the data and a validation DataFrame containing the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = final.randomSplit([0.8, 0.2])\n",
    "adulttrain = splits[0].cache()\n",
    "adultvalid = splits[1].cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a LogisticRegression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of `pyspark.ml.classification.LogisticRegression` called `lr`, set its regularization parameter to 0.01, maximum number of iterations to 500 and `fitIntercept` field to `true`. Then call `fit` using the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adulttrain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6ae70703b4c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregParam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitIntercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlrmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madulttrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'adulttrain' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(regParam=0.01, maxIter=500, fitIntercept=True)\n",
    "lrmodel = lr.fit(adulttrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the model's learned weights (the `coefficients` field) and the intercept value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.020157626786388494,6.603690621609869e-07,0.0001425610141114458,0.000561527929209688,0.026527963778700495,-0.5034520945928904,0.033770285373482804,-0.3853274952722448,0.03521916248055273,-0.10081997017360551,0.23598360344219937,0.5917481429736907,-0.9504456630045122,-1.2939506517324026,-0.3468401927845901,-0.021640247997683545,0.7326485140578827,1.1143948230449094,0.1464011490022018,-0.9457051156688034,0.22953299112509415,-1.0561161912451364,-1.4308120330518177,1.7059261986249328,-1.2411800244199034,-0.6884111713218324,1.5770625796830247,-1.1462311640499518,-1.4807511501074684,-2.117125852067845,0.8365116861077424,-0.6915924629346963,-0.2821544325777778,-0.38737079855242146,-0.25761496103162,-0.20826729870911606,0.6908537419663245,0.19987472349472823,0.031023453933745705,0.6798583305387058,-0.0600088823729738,0.18673554944348694,-0.7433830575106748,-0.2757349372048197,-0.10390864672220759,-0.6002859026788119,-0.7969053670815228,0.47400562398819435,0.3420620283496889,-1.1136513310083271,0.42555307330717745,0.4523664580001107,-0.10433044613140634,-0.7710475853584307,-0.32185943783096677,1.2727272181662956,-0.5362129381527241,0.09561212451233779,-0.1495748411209466,0.19945529240704202,-0.4229480489237599,-0.10728154576740076,0.21207894209430003,-0.6519251030468929,0.30789621825674945,0.20655228110597637,-0.19754278057246163,0.46787852613197034,-0.30783484752226736,0.18404668976976252,0.22090336423779078,0.6016103670561541,-0.39394120308132236,-0.7510422034011426,0.2871164654308411,0.6585842973463751,-1.12103847609592,-0.06512481166756198,-0.4795315159061059,0.05107100549243126,-0.7034129085941284,-1.7058411157665332,-0.30338987157575475,0.5573238586239225,0.05256701728977387,0.19846022849981465,0.09171659407144274,-0.5217512543389554,-0.774771142622846,-0.8019454499756287,0.8180247771915219,1.064791679771553,-0.6408589581669094,-0.44430829616669193,1.0086427805958125,-0.934157647967428,1.355615084427822,-1.071793817708695,-0.556104379577112,-0.20051112671239443,-1.3063778575042375,0.3024097250011646,-1.16818804780908]\n",
      "-4.329714430256324\n"
     ]
    }
   ],
   "source": [
    "print(lrmodel.coefficients)\n",
    "print(lrmodel.intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interpret these values, take a weight value, for example w=0.1928862316, and calculate e^w. The resulting value is the percentage change in odds if the corresponding feature is increased by 1. For example, if the weight value is 1.2127, that means that odds of a person earning more than $50,000 per year increases by 21.27% if that feature increases by 1 (most of these columns are one-hot encoded ones, so this increase 'by 1' actually means 'switch on')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use the model to get predictions for input values. The model is an instance of a `Transformer`, which means it has a `transform` method for transforming DataFrames. Do that now with your validation dataset and inspect the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(103,[0,1,2,4,5,6...|  1.0|[-0.9188485528202...|[0.28519256768000...|       1.0|\n",
      "|(103,[0,1,2,4,5,6...|  0.0|[0.41442085633236...|[0.60214744202577...|       0.0|\n",
      "|(103,[0,1,2,4,5,6...|  0.0|[3.07676852698740...|[0.95592423306525...|       0.0|\n",
      "|(103,[0,1,2,4,5,6...|  0.0|[3.70090307759355...|[0.97589423232617...|       0.0|\n",
      "|(103,[0,1,2,4,5,6...|  0.0|[3.19323563613614...|[0.96057892720889...|       0.0|\n",
      "|(103,[0,1,2,4,5,6...|  0.0|[3.43249273119202...|[0.96870472544821...|       0.0|\n",
      "|(103,[0,1,2,4,5,6...|  0.0|[4.44710823085458...|[0.98842320435030...|       0.0|\n",
      "|(103,[0,1,2,4,5,6...|  0.0|[3.00334907145142...|[0.95272519756421...|       0.0|\n",
      "|(103,[0,1,2,4,5,6...|  0.0|[1.75403073205888...|[0.85246047419474...|       0.0|\n",
      "|(103,[0,1,2,4,5,6...|  0.0|[3.52205801185727...|[0.97130891222445...|       0.0|\n",
      "|(103,[0,1,2,4,5,6...|  0.0|[-2.1938680776126...|[0.10030249081526...|       1.0|\n",
      "|(103,[0,1,2,4,5,6...|  1.0|[0.10245878277574...|[0.52559231102529...|       0.0|\n",
      "|(103,[0,1,2,4,5,6...|  1.0|[-0.4189126134000...|[0.39677698145535...|       1.0|\n",
      "|(103,[0,1,2,4,5,6...|  0.0|[4.63241128494399...|[0.99036251895663...|       0.0|\n",
      "|(103,[0,1,2,4,5,6...|  0.0|[2.73683311230862...|[0.93916541198852...|       0.0|\n",
      "|(103,[0,1,2,4,5,6...|  1.0|[1.02217372925198...|[0.73539579925072...|       0.0|\n",
      "|(103,[0,1,2,4,5,6...|  0.0|[2.25775589789678...|[0.90531744651405...|       0.0|\n",
      "|(103,[0,1,2,4,5,6...|  1.0|[1.26115505344797...|[0.77922487971427...|       0.0|\n",
      "|(103,[0,1,2,4,5,6...|  1.0|[-1.6844647091685...|[0.15650517250779...|       1.0|\n",
      "|(103,[0,1,2,4,5,6...|  1.0|[-0.7453668993590...|[0.32183166721115...|       1.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validpredicts = lrmodel.transform(adultvalid)\n",
    "validpredicts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `probability` column contains vectors with two values: the probability that the sample isn’t in the category (the person is making less than $50,000) and the probability that it is. These two values always add up to 1. The  `rawPrediction` column also contains vectors with two values: the log-odds that a sample doesn’t belong to the category and the log-odds that it does. These two values are always opposite numbers (they add up to 0). The `prediction` column contains 1s and 0s, which indicates whether a sample is likely to belong to the category. A sample is likely to belong to the category if its probability is greater than a certain threshold (0.5 by default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can evaluate the performance of the model using `BinaryClassificationEvaluator`. Just instantiate a new instance and call `evaluate` using your validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9040311371194393"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "bceval = BinaryClassificationEvaluator()\n",
    "bceval.evaluate(validpredicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting value depends on the current metric. Find it out with the `getMetricName` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'areaUnderROC'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bceval.getMetricName()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the metric name to \"areaUnderPR\" and evaluate the model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7580554567790339"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bceval.setMetricName(\"areaUnderPR\")\n",
    "bceval.evaluate(validpredicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methods `pr` and `roc` of the Scala version of the `BinaryClassificationMetrics` class allow you to obtain graph points for precision-recall and receiver-operating curves. Unfortunatelly, this capability is missing from the Python implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-fold cross-validation consists of dividing the dataset into k subsets of equal sizes and training k models excluding a different subset each time. The excluded subset is used as the validation set, and all other subsets are used together as the training set. For each set of parameters you want to validate, you train all k models and then calculate the mean error across all k models (as in figure 8.6). Finally, you choose the set of parameters giving you the smallest average error.\n",
    "\n",
    "Spark's `pyspark.ml.tuning.CrossValidator` class can automate this for you. It needs an *estimator* (a `LogisticRegression` object, for example) and an *evaluator* (`BinaryClassificationEvaluator`, for example), and the number of folds to use.\n",
    "\n",
    "Construct a `CrossValidator` and `set` the needed objects (described above) you constructed before. Specify 5 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv = CrossValidator(estimator=lr, evaluator=bceval, numFolds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build an instance of `pyspark.ml.tuning.ParamGridBuilder`: create a new instance, call `addGrid` for parameters `lr.maxIter` (with one value of 1000) and `lr.regParam` (with values 0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5), and then call `build`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.maxIter, [1000]).\\\n",
    "    addGrid(lr.regParam, [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5]).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, set the resulting `ParamGridBuilder` instance as the value of `estimatorParamMaps` field of your `CrossValidator` and `fit` it on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.setEstimatorParamMaps(paramGrid)\n",
    "cvmodel = cv.fit(adulttrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting model has a `bestModel` field which contains the model with best statistical performance. Examine its `coefficients` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.0222, 0.0, 0.0003, 0.0007, 0.03, -0.6956, -0.4136, -0.916, -0.4334, -0.5776, -0.2629, 0.1881, -1.4715, -4.8562, -0.5909, -0.2194, 0.5937, 1.0053, -0.0685, -1.398, 0.0502, -1.4597, -1.951, 1.7028, -1.6965, -1.0666, 1.5611, -1.616, -2.1614, -11.7542, 1.2528, -1.4578, -0.9816, -1.1204, -0.923, -0.9109, 1.1223, -0.0298, -0.155, 0.5113, -0.2565, -0.0062, -1.0898, -0.4754, -0.2933, -0.8742, -1.1179, 0.3092, 0.1876, -2.3661, 0.308, -0.3914, 0.1259, -0.9891, -0.0657, 0.7056, -0.8472, -0.6744, -0.9351, -0.4725, -1.2847, -0.8483, -0.9671, -1.9029, -0.9007, -0.9924, -1.3611, -0.6444, -1.5826, -1.1403, -0.8561, -0.5944, -1.7426, -2.155, -0.7574, -0.4024, -2.6014, -1.4475, -1.6589, -1.0907, -2.1497, -3.5133, -1.4285, -0.3714, -1.2017, -0.934, -1.2177, -1.6934, -2.0246, -2.2358, -0.2823, 0.0647, -1.9894, -1.7176, -0.227, -2.2883, 0.3142, -2.5081, -1.7239, -1.4792, -5.4461, -0.7987, -4.7835])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvmodel.bestModel.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use this best model to transform the validation dataset and validate it using a new `BinaryClassificationEvaluator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9063025251654941"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BinaryClassificationEvaluator().evaluate(cvmodel.bestModel.transform(adultvalid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result should be better than the previous result you obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said earlier, multiclass classification means a classifier categorizes input examples into several classes. One of the options for performing multiclass classification in Spark is the *one vs. rest strategy*. When using the one vs. rest strategy, you train one model per class, each time treating all other classes (the rest) as negatives. Then, when classifying new samples, you classify them using all the trained models and pick the class corresponding to the\n",
    "model that gives the highest probability. Spark ML provides the `pyspark.ml.classification.OneVsRest` class precisely for this purpose. It produces a `OneVsRestModel` that you can use for dataset transformation. You can use the `MulticlassMetrics` class from MLlib for evaluating the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example dataset you will use the data extracted from scaled images of handwritten numbers. It’s a public dataset available from the UCI machine learning repository, containing 10,992 samples of handwritten digits from 0 to 9. Each sample contains 16 pixels with intensity values of 0–100.\n",
    "\n",
    "Use the following code to load the dataset and split it into the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpen = sc.textFile(\"../first-edition/ch08/penbased.dat\", 4).map(lambda x: x.split(\", \")).\\\n",
    "    map(lambda row: [int(float(x)) for x in row]).map(lambda raw: Row(*raw)).\\\n",
    "    toDF(['pix1', 'pix2', 'pix3', 'pix4', 'pix5', 'pix6', 'pix7', 'pix8', 'pix9', 'pix10', \n",
    "         'pix11', 'pix12', 'pix13', 'pix14', 'pix15', 'pix16', 'label'])\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "va = VectorAssembler().setOutputCol(\"features\")\n",
    "cols = dfpen.columns\n",
    "cols.remove('label')\n",
    "va.setInputCols(cols)\n",
    "penlpoints = va.transform(dfpen).select(\"features\", \"label\")\n",
    "\n",
    "pensets = penlpoints.randomSplit([0.8, 0.2])\n",
    "pentrain = pensets[0].cache()\n",
    "penvalid = pensets[1].cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `LogisticRegression` classifier with a regularization parameter of 0.01. Then create a `OneVsRest` object and give it this classifier using the `setClassifier` method. Then call the `fit` method of the `OneVsRest` object using the `pentrain` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import OneVsRest\n",
    "penlr = LogisticRegression(regParam=0.01)\n",
    "ovrest = OneVsRest()\n",
    "ovrest.setClassifier(penlr)\n",
    "ovrestmodel = ovrest.fit(pentrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the resulting model to transform the validation dataset and convert the result into an RDD containing prediction-label tuples. This is needed because you will use `pyspark.mllib.evaluation.MulticlassMetrics` evaluator from the MLlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "penresult = ovrestmodel.transform(penvalid)\n",
    "penPreds = penresult.select(\"prediction\", \"label\").\\\n",
    "    rdd.map(lambda row: (float(row[0]), float(row[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, provide the resulting RDD to the `MulticlassMetrics` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8922339278852328"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "mce = MulticlassMetrics(penPreds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now access `precision` and `recall` metrics for each class by calling methods with those names and providing the index of the class you want. To that now for several classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8981481481481481\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(mce.precision(3))\n",
    "print(mce.recall(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MulticlassMetrics`'s `confusionMatrix` is a good way to visualize the results. Print it out now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[201.,   3.,   0.,   0.,   2.,   0.,   2.,   0.,  19.,   1.],\n",
      "             [  0., 133.,  28.,   5.,   0.,  16.,   0.,   1.,   0.,   0.],\n",
      "             [  0.,   6., 168.,   0.,   0.,   1.,   0.,   4.,   0.,   0.],\n",
      "             [  0.,   0.,   0., 194.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "             [  0.,   1.,   0.,   0., 188.,   1.,   1.,   0.,   0.,   2.],\n",
      "             [  0.,   0.,   1.,  12.,   0., 120.,  11.,   5.,   3.,  29.],\n",
      "             [  0.,   0.,   0.,   0.,   1.,   0., 181.,   0.,   4.,   0.],\n",
      "             [  0.,  10.,   2.,   3.,   1.,   4.,   0., 201.,   1.,   5.],\n",
      "             [ 10.,   6.,   0.,   0.,   0.,   4.,   0.,   1., 168.,   0.],\n",
      "             [  2.,   7.,   0.,   2.,   2.,   3.,   0.,   0.,   0., 175.]])\n"
     ]
    }
   ],
   "source": [
    "print(mce.confusionMatrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will train a decision tree and random forests models on the same dataset, but we will first string-index it. Please execute this code to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtsi = StringIndexer(inputCol=\"label\", outputCol=\"label-i\")\n",
    "dtsm = dtsi.fit(penlpoints)\n",
    "pendtlpoints = dtsm.transform(penlpoints).drop(\"label\").\\\n",
    "    withColumnRenamed(\"label-i\", \"label\")\n",
    "pendtsets = pendtlpoints.randomSplit([0.8, 0.2])\n",
    "pendttrain = pendtsets[0].cache()\n",
    "pendtvalid = pendtsets[1].cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of `pyspark.ml.classification.DecisionTreeClassifier`, set its `maxDepth` to 20 and call `fit` using `pendttrain` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(maxDepth=20)\n",
    "dtmodel = dt.fit(pendttrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Scala, you can examine the decisions the model makes by traversing its decision tree from the `rootNode` all the way to the leaf nodes.\n",
    "\n",
    "However, this feature is not available in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate the model by transforming the validation set (`pendtvalid`), again creating an RDD of prediction-value tuples and using a `MulticlassMetrics` instance. Print out the resulting precision and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9601468274777137\n",
      "DenseMatrix([[189.,   0.,   0.,   0.,   0.,   0.,   1.,   2.,   1.,   1.],\n",
      "             [  0., 203.,   0.,   2.,   2.,   1.,   0.,   0.,   0.,   1.],\n",
      "             [  0.,   0., 171.,   0.,   0.,   0.,   0.,   5.,   0.,   0.],\n",
      "             [  0.,   2.,   0., 193.,   6.,   3.,   1.,   3.,   1.,   3.],\n",
      "             [  0.,   0.,   0.,  12., 182.,   0.,   0.,   0.,   0.,   0.],\n",
      "             [  0.,   0.,   1.,   0.,   0., 187.,   0.,   1.,   0.,   0.],\n",
      "             [  2.,   1.,   0.,   0.,   0.,   0., 163.,   1.,   3.,   0.],\n",
      "             [  0.,   0.,   2.,   0.,   0.,   0.,   3., 178.,   3.,   0.],\n",
      "             [  0.,   0.,   0.,   2.,   0.,   0.,   0.,   1., 183.,   0.],\n",
      "             [  0.,   0.,   0.,   3.,   1.,   0.,   0.,   1.,   4., 182.]])\n"
     ]
    }
   ],
   "source": [
    "dtpredicts = dtmodel.transform(pendtvalid)\n",
    "dtresrdd = dtpredicts.select(\"prediction\", \"label\").rdd.map(lambda row: (float(row[0]), float(row[1])))\n",
    "dtmm = MulticlassMetrics(dtresrdd)\n",
    "print(dtmm.precision())\n",
    "print(dtmm.confusionMatrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the results better than those obtained using logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random-forests algorithm is an ensemble method of training a number of decision trees and selecting the best result by averaging results from all of them. This enables the algorithm to avoid overfitting and to find a global optima that particular decision trees can’t find on their own.\n",
    "\n",
    "Random forests in Spark are implemented by the classes `RandomForestClassifier` and `RandomForestRegressor` (here we will be using the classification version). You can configure it with two additional parameters (besides `maxDepth` you already saw being used for decision trees): `numTrees` (the number of trees to train; the default is 20) and `featureSubsetStrategy` (determines how feature bagging is done). The defaults work fine in most cases.\n",
    "\n",
    "Create a new instance of `RandomForestClassifier`, set its `maxDepth` to 20 and `fit` it on the `pendttrain` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(maxDepth=20)\n",
    "rfmodel = rf.fit(pendttrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting model has the `trees` field containing the trees it has trained. Access it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DecisionTreeClassificationModel (uid=dtc_d531958eab76) of depth 16 with 681 nodes,\n",
       " DecisionTreeClassificationModel (uid=dtc_698c6240285a) of depth 20 with 757 nodes,\n",
       " DecisionTreeClassificationModel (uid=dtc_4e0734765111) of depth 15 with 637 nodes,\n",
       " DecisionTreeClassificationModel (uid=dtc_4c4bfaf01bca) of depth 18 with 671 nodes,\n",
       " DecisionTreeClassificationModel (uid=dtc_4f88bbab552c) of depth 17 with 713 nodes,\n",
       " DecisionTreeClassificationModel (uid=dtc_7ae7f53fdc69) of depth 16 with 765 nodes,\n",
       " DecisionTreeClassificationModel (uid=dtc_7d14e3004158) of depth 18 with 707 nodes,\n",
       " DecisionTreeClassificationModel (uid=dtc_400a81ca6dc1) of depth 20 with 811 nodes,\n",
       " DecisionTreeClassificationModel (uid=dtc_4e548f1bd104) of depth 20 with 717 nodes,\n",
       " DecisionTreeClassificationModel (uid=dtc_cac58538d0b1) of depth 17 with 679 nodes,\n",
       " DecisionTreeClassificationModel (uid=dtc_671ed323f30e) of depth 15 with 759 nodes,\n",
       " DecisionTreeClassificationModel (uid=dtc_6f2d951d886c) of depth 19 with 671 nodes,\n",
       " DecisionTreeClassificationModel (uid=dtc_7879b7c9dbda) of depth 18 with 749 nodes,\n",
       " DecisionTreeClassificationModel (uid=dtc_fd9969279169) of depth 16 with 723 nodes,\n",
       " DecisionTreeClassificationModel (uid=dtc_deaf4dda9c00) of depth 17 with 745 nodes,\n",
       " DecisionTreeClassificationModel (uid=dtc_634cb361fe20) of depth 16 with 715 nodes,\n",
       " DecisionTreeClassificationModel (uid=dtc_617fc14575cc) of depth 16 with 743 nodes,\n",
       " DecisionTreeClassificationModel (uid=dtc_ec31de596de6) of depth 16 with 727 nodes,\n",
       " DecisionTreeClassificationModel (uid=dtc_049b0af55fbd) of depth 16 with 643 nodes,\n",
       " DecisionTreeClassificationModel (uid=dtc_10e9201aa1e2) of depth 17 with 699 nodes]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfmodel.trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the resulting model is just another `Transformer` so you can use it to transform the validation dataset similarly to what you did previously. Do that now and then use a `MulticlassMetrics` instance to obtain its precision and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9884635553224961\n",
      "DenseMatrix([[193.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.],\n",
      "             [  0., 208.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,   0.],\n",
      "             [  0.,   0., 175.,   0.,   0.,   0.,   0.,   1.,   0.,   0.],\n",
      "             [  0.,   2.,   0., 203.,   5.,   0.,   0.,   0.,   0.,   2.],\n",
      "             [  0.,   1.,   0.,   2., 191.,   0.,   0.,   0.,   0.,   0.],\n",
      "             [  0.,   0.,   0.,   0.,   0., 189.,   0.,   0.,   0.,   0.],\n",
      "             [  0.,   0.,   0.,   0.,   0.,   0., 169.,   0.,   1.,   0.],\n",
      "             [  0.,   1.,   0.,   0.,   0.,   0.,   0., 185.,   0.,   0.],\n",
      "             [  0.,   0.,   0.,   0.,   0.,   0.,   1.,   2., 183.,   0.],\n",
      "             [  0.,   0.,   0.,   1.,   1.,   0.,   0.,   0.,   0., 189.]])\n"
     ]
    }
   ],
   "source": [
    "rfpredicts = rfmodel.transform(pendtvalid)\n",
    "rfresrdd = rfpredicts.select(\"prediction\", \"label\").rdd.map(lambda row: (float(row[0]), float(row[1])))\n",
    "rfmm = MulticlassMetrics(rfresrdd)\n",
    "print(rfmm.precision())\n",
    "print(rfmm.confusionMatrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering is the simplest and the most often used of the three. Unfortunately, it has drawbacks: it has trouble handling non-spherical clusters and unevenly sized clusters (uneven by density or by radius). It also can’t make efficient use of the one-hotencoded features you used in section 8.2.2. It’s often used for classifying text documents, along with the term frequency-inverse document frequency (TF-IDF) featurevectorization method.\n",
    "\n",
    "Each image of handwritten digits, which you’ll use for this example, is represented as a series of numbers (dimensions) representing image pixels. As such, each image is a point in an n-dimensional space. K-means clustering can group together images that are close in this space. In an ideal case, all of these will be images of the same digit.\n",
    "\n",
    "To implement k-means, you first have to make sure your dataset is standardized (all dimensions are of comparable ranges), because k-means clustering doesn’t work well with non-standardized data. The dimensions of the handwritten digit dataset are already standardized (all the values go from 0 to 100), so you can skip this step now. But with clustering algorithms, there’s no point in having a validation and a training dataset. So, you’ll use the entire dataset contained in the penlpoints DataFrame you used before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KMeans estimator can be parameterized with the following parameters:\n",
    "- k — Number of clusters to find (default is 2)\n",
    "- maxIter — Maximum number of iterations to perform (required).\n",
    "- predictionCol — Prediction column name (default is “prediction”)\n",
    "- featuresCol — Features column name (default is “features”)\n",
    "- tol — Convergence tolerance\n",
    "- seed — Random seed value for cluster initialization\n",
    "\n",
    "Create a new instance of `pyspark.ml.clustering.KMeans`, set `k` to 10 (there are 10 digits in the dataset) and `maxIter` to 500. Then use it to fit a model with the `penlpoints` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "kmeans = KMeans(k=10, maxIter=500)\n",
    "kmmodel = kmeans.fit(penlpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cell to define the `printContingency` function which can output a contingency table with the original labels as rows and k-means cluster indexes as columns. The cells in the table contain counts of examples belonging both to the original label and the predicted cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd contains tuples (prediction, label)\n",
    "def printContingency(rdd, labels):\n",
    "    import operator\n",
    "    numl = len(labels)\n",
    "    tablew = 6*numl + 10\n",
    "    divider = \"----------\"\n",
    "    for l in labels:\n",
    "        divider += \"+-----\"\n",
    "    summ = 0\n",
    "    print(\"orig.class\", end='')\n",
    "    for l in labels:\n",
    "        print(\"|Pred\"+str(l), end='')\n",
    "    print()\n",
    "    print(divider)\n",
    "    labelMap = {}\n",
    "    for l in labels:\n",
    "        #filtering by predicted labels\n",
    "        predCounts = rdd.filter(lambda p:  p[1] == l).countByKey()\n",
    "        #get the cluster with most elements\n",
    "        topLabelCount = sorted(predCounts.items(), key=operator.itemgetter(1), reverse=True)[0]\n",
    "        #if there are two (or more) clusters for the same label\n",
    "        if(topLabelCount[0] in labelMap):\n",
    "            #and the other cluster has fewer elements, replace it\n",
    "            if(labelMap[topLabelCount[0]][1] < topLabelCount[1]):\n",
    "                summ -= labelMap[l][1]\n",
    "                labelMap.update({topLabelCount[0]: (l, topLabelCount[1])})\n",
    "                summ += topLabelCount[1]\n",
    "            #else leave the previous cluster in\n",
    "        else:\n",
    "            labelMap.update({topLabelCount[0]: (l, topLabelCount[1])})\n",
    "            summ += topLabelCount[1]\n",
    "        predictions = iter(sorted(predCounts.items(), key=operator.itemgetter(0)))\n",
    "        predcount = next(predictions)\n",
    "        print(\"%6d    \" % (l), end='')\n",
    "        for predl in labels:\n",
    "            if(predcount[0] == predl):\n",
    "                print(\"|%5d\" % (predcount[1]), end='')\n",
    "                try:\n",
    "                    predcount = next(predictions)\n",
    "                except:\n",
    "                    pass\n",
    "            else:\n",
    "                print(\"|    0\", end='')\n",
    "        print()\n",
    "        print(divider)\n",
    "    print(\"Purity: %s\" % (float(summ)/rdd.count()))\n",
    "    print(\"Predicted->original label map: %s\" % str([str(x[0])+\": \"+str(x[1][0]) for x in labelMap.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method takes an RDD containing tuples with predictions and the original labels (both double values). Call the model's `transform` method on `penlpoints` DataFrame, then obtain the RDD with the `rdd` field, and then call the function using the RDD as the first argument and 0 to 9 range as the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig.class|Pred0|Pred1|Pred2|Pred3|Pred4|Pred5|Pred6|Pred7|Pred8\n",
      "----------+-----+-----+-----+-----+-----+-----+-----+-----+-----\n",
      "     0    |    0|    3|    6|   30|    7|    0|  638|    0|  353\n",
      "----------+-----+-----+-----+-----+-----+-----+-----+-----+-----\n",
      "     1    |   21|  574|  282|    8|    1|   70|    0|   66|    0\n",
      "----------+-----+-----+-----+-----+-----+-----+-----+-----+-----\n",
      "     2    |    2|   15| 1003|    0|    0|    2|    0|    0|    0\n",
      "----------+-----+-----+-----+-----+-----+-----+-----+-----+-----\n",
      "     3    |    0|   19|    1|    0|    1|  919|    0|    2|    0\n",
      "----------+-----+-----+-----+-----+-----+-----+-----+-----+-----\n",
      "     4    |    0|   12|    4|   41|  938|    1|    0|   31|    0\n",
      "----------+-----+-----+-----+-----+-----+-----+-----+-----+-----\n",
      "     5    |    0|    0|    0|    6|    0|  211|    0|  175|    0\n",
      "----------+-----+-----+-----+-----+-----+-----+-----+-----+-----\n",
      "     6    |    0|    0|    0|  965|    3|    0|    0|    0|    0\n",
      "----------+-----+-----+-----+-----+-----+-----+-----+-----+-----\n",
      "     7    |  800|  146|    7|    1|    1|   69|    0|    0|    0\n",
      "----------+-----+-----+-----+-----+-----+-----+-----+-----+-----\n",
      "     8    |  259|    0|   23|   11|    0|   87|   15|    4|  389\n",
      "----------+-----+-----+-----+-----+-----+-----+-----+-----+-----\n",
      "Purity: 0.6844229217110573\n",
      "Predicted->original label map: ['6: 0', '1: 1', '2: 2', '5: 3', '4: 4', '9: 5', '3: 6', '0: 7', '8: 8']\n"
     ]
    }
   ],
   "source": [
    "kmpredicts = kmmodel.transform(penlpoints)\n",
    "printContingency(kmpredicts.select(\"prediction\", \"label\").rdd, range(0, 9))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
