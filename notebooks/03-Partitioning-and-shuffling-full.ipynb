{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning and shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, execute the following cell to create a Spark context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(master='local[*]', appName=\"Spark course Partitioning and Shuffling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an RDD containing numbers from 1 to 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.range(1, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many partitions does it have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many elements does each partition have? (Use `mapPartitions` to create an RDD called `partSizes` and then call `collect` on it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2499, 2500, 2500, 2500]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mapfunc(iter):\n",
    "    print(\"mappartitions\")\n",
    "    return [len(iter)]\n",
    "partSizes = rdd.mapPartitions(mapfunc)\n",
    "partSizes.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map `partSizes` RDD to tuples with `lambda x: (x, x*x)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = partSizes.map(lambda x: (x, x*x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will this RDD cause a shuffle? (Find out by examining RDD's lineage.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(4) PythonRDD[9] at RDD at PythonRDD.scala:50 []\n",
      " |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:185 []'\n"
     ]
    }
   ],
   "source": [
    "print(str(rdd2.toDebugString()).replace('\\\\n', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce this RDD by key by summing up values and call the resulting RDD `reducedRdd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedRdd = rdd2.reduceByKey(lambda v1, v2: v1+v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will this last RDD cause a shuffle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(4) PythonRDD[23] at RDD at PythonRDD.scala:50 []\n",
      " |  MapPartitionsRDD[22] at mapPartitions at PythonRDD.scala:130 []\n",
      " |  ShuffledRDD[21] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(4) PairwiseRDD[20] at reduceByKey at <ipython-input-20-8eb780c4b668>:1 []\n",
      "    |  PythonRDD[19] at reduceByKey at <ipython-input-20-8eb780c4b668>:1 []\n",
      "    |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:185 []'\n"
     ]
    }
   ],
   "source": [
    "print(str(reducedRdd.toDebugString()).replace('\\\\n', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Spark's checkpoint directory to `/home/spark/checkpointDir` then checkpoint the `reducedRdd` RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setCheckpointDir('/home/spark/checkpointDir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreate the `reducedRdd` but give it a different name (copy the line from the previous cell where you did this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducedRdd2 = rdd2.reduceByKey(lambda v1, v2: v1+v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine its lineage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(4) PythonRDD[37] at RDD at PythonRDD.scala:50 []\n",
      " |  MapPartitionsRDD[36] at mapPartitions at PythonRDD.scala:130 []\n",
      " |  ShuffledRDD[35] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(4) PairwiseRDD[34] at reduceByKey at <ipython-input-34-6ff29a7a73ef>:1 []\n",
      "    |  PythonRDD[33] at reduceByKey at <ipython-input-34-6ff29a7a73ef>:1 []\n",
      "    |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:185 []'\n"
     ]
    }
   ],
   "source": [
    "print(str(reducedRdd2.toDebugString()).replace('\\\\n', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint this RDD and examine its lineage again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(4) PythonRDD[37] at RDD at PythonRDD.scala:50 []\n",
      " |  MapPartitionsRDD[36] at mapPartitions at PythonRDD.scala:130 []\n",
      " |  ShuffledRDD[35] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(4) PairwiseRDD[34] at reduceByKey at <ipython-input-34-6ff29a7a73ef>:1 []\n",
      "    |  PythonRDD[33] at reduceByKey at <ipython-input-34-6ff29a7a73ef>:1 []\n",
      "    |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:185 []'\n"
     ]
    }
   ],
   "source": [
    "reducedRdd2.checkpoint()\n",
    "print(str(reducedRdd2.toDebugString()).replace('\\\\n', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now \"materialize\" the RDD by calling an action on it (such as `count`) and reexamine the lineage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reducedRdd2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(4) PythonRDD[37] at RDD at PythonRDD.scala:50 []\n",
      " |  ReliableCheckpointRDD[39] at count at <ipython-input-37-ebf95e46e539>:1 []'\n"
     ]
    }
   ],
   "source": [
    "print(str(reducedRdd2.toDebugString()).replace('\\\\n', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You had to create a new `reducedRdd` because the previous one was already materialized and checkpointing it would have no effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an RDD containing a few thousand numbers and map its partitions (`mapPartitions`) with a function that returns number of elements in each partition but also prints that information out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.range(0, 10000)\n",
    "def mapfunc(itr):\n",
    "    print(str(len(itr)))\n",
    "    return [len(itr)]\n",
    "rdd2 = rdd.mapPartitions(mapfunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now \"materialize\" this RDD by calling an action on it (such as `collect` or `count`). Execute that cell several times and watch the output from the Jupyter server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2500, 2500, 2500, 2500]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now cache this RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[8] at collect at <ipython-input-12-83517eaf6d43>:1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And call the same action several times again. Do you see the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you no longer need the cached RDD you should always `unpersist` it explicitly because keeping it in memory might cause increased garbage collection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
