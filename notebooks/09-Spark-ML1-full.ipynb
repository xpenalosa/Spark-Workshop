{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark ML - Part One"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing anything else, let's create a Spark context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(master='local[*]', appName=\"Spark course Pair RDDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For learning how to do linear regression in Spark we will use the housing dataset (https://archive.ics.uci.edu/ml/datasets/Housing). The dataset contains mean values of owner-occupied homes in the suburbs of Boston and 13 features that can be used to predict home values. The data contains the following columns:\n",
    "1. CRIM      per capita crime rate by town\n",
    "2. ZN        proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "3. INDUS     proportion of non-retail business acres per town\n",
    "4. CHAS      Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "5. NOX       nitric oxides concentration (parts per 10 million)\n",
    "6. RM        average number of rooms per dwelling\n",
    "7. AGE       proportion of owner-occupied units built prior to 1940\n",
    "8. DIS       weighted distances to five Boston employment centres\n",
    "9. RAD       index of accessibility to radial highways\n",
    "10. TAX      full-value property-tax rate per \\$10,000\n",
    "11. PTRATIO  pupil-teacher ratio by town\n",
    "12. B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "13. LSTAT    % lower status of the population\n",
    "14. MEDV Median value of owner-occupied homes in $1000's\n",
    "\n",
    "Load the data using this snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors, Vector\n",
    "housingLines = sc.textFile(\"/home/spark/first-edition/ch07/housing.data\", 4)\n",
    "housingVals = housingLines.map(lambda x: Vectors.dense([float(v.strip()) for v in x.split(\",\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate multivariate statistical summary by creating a `pyspark.mllib.linalg.distributed.RowMatrix` variable called *housingMat* from `housingVals` RDD and then call its `computeColumnSummaryStatistics` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "housingMat = RowMatrix(housingVals)\n",
    "housingStats = housingMat.computeColumnSummaryStatistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the resulting `MultivariateStatisticalSummary` object to find the minimum, maximum and mean values of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.3200e-03 0.0000e+00 4.6000e-01 0.0000e+00 3.8500e-01 3.5610e+00\n",
      " 2.9000e+00 1.1296e+00 1.0000e+00 1.8700e+02 1.2600e+01 3.2000e-01\n",
      " 1.7300e+00 5.0000e+00]\n",
      "[ 88.9762 100.      27.74     1.       0.871    8.78   100.      12.1265\n",
      "  24.     711.      22.     396.9     37.97    50.    ]\n",
      "[3.61352356e+00 1.13636364e+01 1.11367787e+01 6.91699605e-02\n",
      " 5.54695059e-01 6.28463439e+00 6.85749012e+01 3.79504269e+00\n",
      " 9.54940711e+00 4.08237154e+02 1.84555336e+01 3.56674032e+02\n",
      " 1.26530632e+01 2.25328063e+01]\n"
     ]
    }
   ],
   "source": [
    "print(housingStats.min())\n",
    "print(housingStats.max())\n",
    "print(housingStats.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the L1 norm (method `normL1`), Euclidian norm (method `normL2`) and variance of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.82844292e+03 5.75000000e+03 5.63521000e+03 3.50000000e+01\n",
      " 2.80675700e+02 3.18002500e+03 3.46989000e+04 1.92029160e+03\n",
      " 4.83200000e+03 2.06568000e+05 9.33850000e+03 1.80477060e+05\n",
      " 6.40245000e+03 1.14016000e+04]\n",
      "[2.09691067e+02 5.83120056e+02 2.94152392e+02 5.91607978e+00\n",
      " 1.27463869e+01 1.42248368e+02 1.66721763e+03 9.76051548e+01\n",
      " 2.90568408e+02 9.93343526e+03 4.17987954e+02 8.28133627e+03\n",
      " 3.26746015e+02 5.47381348e+02]\n",
      "[7.39865782e+01 5.43936814e+02 4.70644425e+01 6.45129730e-02\n",
      " 1.34276357e-02 4.93670850e-01 7.92358399e+02 4.43401514e+00\n",
      " 7.58163660e+01 2.84047595e+04 4.68698912e+00 8.33475226e+03\n",
      " 5.09947595e+01 8.45867236e+01]\n"
     ]
    }
   ],
   "source": [
    "print(housingStats.normL1())\n",
    "print(housingStats.normL2())\n",
    "print(housingStats.variance())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cell to define the helper method that will serve the purpose of printing out matrices in a more readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printMat(mat):\n",
    "    if hasattr(mat, 'rows'):\n",
    "        vals = mat.rows.collect()\n",
    "        numrows = mat.numRows()\n",
    "        numcols = mat.numCols()\n",
    "    elif hasattr(mat, 'toRowMatrix'):\n",
    "        rm = mat.toRowMatrix()\n",
    "        vals = rm.rows.collect()\n",
    "        numrows, numcols = (rm.numRows(), rm.numCols())\n",
    "    else:\n",
    "        numrows, numcols = (housingCovar.numRows, housingCovar.numCols)\n",
    "        vals = housingCovar.values.reshape((numrows, numcols))\n",
    "    \n",
    "    print(\"   \" + \"\".join([(\"% 10d\" % j) for j in range(numcols)]))\n",
    "    for i in range(numrows):\n",
    "        print((\"%-6d\" % i) + \"\".join([\"% 10.3f\" % vals[i][j] for j in range(numcols)]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `columnSimilarities` method on the *housingMat* matrix and then print it out using the `printMat` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0         1         2         3         4         5         6         7         8         9        10        11        12        13\n",
      "0          0.000     0.000     0.000     0.000     0.000     0.966     0.962     0.780     0.808     0.957     0.977     0.929     0.912     0.873\n",
      "1          0.000     0.004     0.527     0.052     0.459     0.363     0.482     0.169     0.675     0.563     0.416     0.288     0.544     0.224\n",
      "2          0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.917     0.771     0.642     0.806     0.588\n",
      "3          0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.670\n",
      "4          0.000     0.000     0.122     0.078     0.334     0.467     0.211     0.673     0.135     0.297     0.394     0.464     0.200     0.528\n",
      "5          0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.939     0.854     0.907     0.789\n",
      "6          0.000     0.000     0.000     0.000     0.000     0.000     0.909     0.880     0.719     0.906     0.982     0.966     0.832     0.949\n",
      "7          0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.672     0.801     0.929     0.930     0.871     0.918     0.803\n",
      "8          0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.957     0.887     0.897\n",
      "9          0.000     0.000     0.000     0.256     0.915     0.824     0.916     0.565     0.840     0.931     0.869     0.779     0.897     0.693\n",
      "10         0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.799     0.928\n",
      "11         0.000     0.000     0.000     0.000     0.275     0.271     0.275     0.184     0.190     0.230     0.248     0.266     0.204     0.307\n",
      "12         0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.000     0.485     0.710     0.856     0.882     0.644     0.856\n"
     ]
    }
   ],
   "source": [
    "housingColSims = housingMat.columnSimilarities()\n",
    "printMat(housingColSims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the values in the cells of this matrix, the more correlated the two columns are. Negative values indicate opposite correlation. The highest value in the last column (corresponding to the target price) is in the sixth row, which corresponds to the average number of rooms. That is the feature that correlates the most with the price of a house."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, examine similarities between columns by computing a covariance matrix using the `computeCovariance` method on the *housingMat* matrix. Print out the resulting matrix as you did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0         1         2         3         4         5         6         7         8         9        10        11        12        13\n",
      "0         73.987   -40.216    23.992    -0.122     0.420    -1.325    85.405    -6.877    46.848   844.822     5.399  -302.382    27.986   -30.719\n",
      "1        -40.216   543.937   -85.413    -0.253    -1.396     5.113  -373.902    32.629   -63.349 -1236.454   -19.777   373.721   -68.783    77.315\n",
      "2         23.992   -85.413    47.064     0.110     0.607    -1.888   124.514   -10.228    35.550   833.360     5.692  -223.580    29.580   -30.521\n",
      "3         -0.122    -0.253     0.110     0.065     0.003     0.016     0.619    -0.053    -0.016    -1.523    -0.067     1.131    -0.098     0.409\n",
      "4          0.420    -1.396     0.607     0.003     0.013    -0.025     2.386    -0.188     0.617    13.046     0.047    -4.021     0.489    -0.455\n",
      "5         -1.325     5.113    -1.888     0.016    -0.025     0.494    -4.752     0.304    -1.284   -34.583    -0.541     8.215    -3.080     4.493\n",
      "6         85.405  -373.902   124.514     0.619     2.386    -4.752   792.358   -44.329   111.771  2402.690    15.937  -702.940   121.078   -97.589\n",
      "7         -6.877    32.629   -10.228    -0.053    -0.188     0.304   -44.329     4.434    -9.068  -189.665    -1.060    56.040    -7.473     4.840\n",
      "8         46.848   -63.349    35.550    -0.016     0.617    -1.284   111.771    -9.068    75.816  1335.757     8.761  -353.276    30.385   -30.561\n",
      "9        844.822 -1236.454   833.360    -1.523    13.046   -34.583  2402.690  -189.665  1335.757 28404.759   168.153 -6797.911   654.715  -726.256\n",
      "10         5.399   -19.777     5.692    -0.067     0.047    -0.541    15.937    -1.060     8.761   168.153     4.687   -35.060     5.783   -10.111\n",
      "11      -302.382   373.721  -223.580     1.131    -4.021     8.215  -702.940    56.040  -353.276 -6797.911   -35.060  8334.752  -238.668   279.990\n",
      "12        27.986   -68.783    29.580    -0.098     0.489    -3.080   121.078    -7.473    30.385   654.715     5.783  -238.668    50.995   -48.448\n",
      "13       -30.719    77.315   -30.521     0.409    -0.455     4.493   -97.589     4.840   -30.561  -726.256   -10.111   279.990   -48.448    84.587\n"
     ]
    }
   ],
   "source": [
    "housingCovar = housingMat.computeCovariance()\n",
    "printMat(housingCovar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the data in Spark, you need to put each example in the dataset in a structure called a `LabeledPoint`, which is used in most of Spark’s machine-learning algorithms. The `LabeledPoint` constructor takes the label (the last element in our dataset) as the first argument, and a `DenseVector`  of features (construct it by calling `Vectors.dense`) as the second argument. Transform the `housingVals` RDD like this now. Call the resulting variable `housingData`. \n",
    "\n",
    "(Hint: first convert the Vectors in the RDD to arrays by calling `toArray` and then *index into* and `slice` the arrays.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def vectorToLPoint(v):\n",
    "    a = v.toArray()\n",
    "    return LabeledPoint(a[-1], Vectors.dense(a[0:-1]))\n",
    "\n",
    "housingData = housingVals.map(vectorToLPoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `randomSplit` method to split the `housingData` RDD into two new RDDs, `housingTrain` and `housingValid`, containing 80% and 20% of data, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = housingData.randomSplit([0.8, 0.2])\n",
    "housingTrain = sets[0]\n",
    "housingValid = sets[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create two RDDs containing only `features` from `housingTrain` and `housingValid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresTrain = housingTrain.map(lambda lp: lp.features)\n",
    "featuresValid = housingValid.map(lambda lp: lp.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to scale the data so that all features are in the same range (*feature scaling*) and that their averages are roughly zero (*Mean normalization*). Create an instance of `pyspark.mllib.feature.StandardScaler` and specify that you want to do both standardization techniques. Then `fit` the scaler on the RDD containing features from `housingTrain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import StandardScaler\n",
    "scaler = StandardScaler(True, True).fit(featuresTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform both feature RDDs using the fitted scaler and then `zip` the results with the labels from `housingTrain` and `housingValid` RDDs. Finally, transform those results into `LabeledPoint`s and call the resulting RDDs `trainScaled` and `validScaled`. Cache the two resulting RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainScaled = housingTrain.map(lambda lp: lp.label).zip(scaler.transform(featuresTrain)).\\\n",
    "  map(lambda lft: LabeledPoint(lft[0], lft[1])).cache()\n",
    "validScaled = housingValid.map(lambda lp: lp.label).zip(scaler.transform(featuresValid)).\\\n",
    "  map(lambda lft: LabeledPoint(lft[0], lft[1])).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and using a linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use the prepared data to train a linear regression model. Create a new `pyspark.mllib.regression.LinearRegressionWithSGD` object and train it on `trainScaled` RDD, with the `intercept` flag set to `True` and with 200 iterations. Save the resulting model as a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "model = LinearRegressionWithSGD.train(trainScaled, iterations=200, intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use the trained model to predict the target values of vectors in the validation set by running predict on every element. Map `validScaled` RDD's elements to tuples containing the predicted value (obtained using the model's `predict` method on the features) and the original label. Important: make sure to convert both values to `float`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "validPredicts = validScaled.map(lambda x: (float(model.predict(x.features)), float(x.label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the root mean squared error (square root of sum of squares of differences between original and predicted target values) using Spark and Python APIs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, l = validPredicts.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.335340896925252"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.sqrt(validPredicts.map(lambda pl: pow(pl[0]-pl[1],2)).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average value of the target variables (home prices) is 22.5 so your root mean squared error may seem rather large. But take into account that the variance of home prices is 84.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use `pyspark.mllib.evaluation.RegressionMetrics` evaluate the performance of your regression model. Instantiate it with the RDD containing predictions and then read `rootMeanSquaredError`, `meanSquaredError`, `meanAbsoluteError`, `r2` and `explainedVariance` fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.335340896925252\n",
      "18.795180692552645\n",
      "3.078082986627338\n",
      "0.7739983577491686\n",
      "54.03379290540022\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "validMetrics = RegressionMetrics(validPredicts)\n",
    "print(validMetrics.rootMeanSquaredError)\n",
    "print(validMetrics.meanSquaredError)\n",
    "print(validMetrics.meanAbsoluteError)\n",
    "print(validMetrics.r2)\n",
    "print(validMetrics.explainedVariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read model's weights and sort them by their absolute values to see which features are the most important (you can convert weights to a list with the `toArray` method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.2098393251764806, 6),\n",
       " (0.24366959965218268, 2),\n",
       " (0.5965668065825196, 3),\n",
       " (0.7266956603940284, 1),\n",
       " (0.7378968231306785, 0),\n",
       " (0.7628197291548122, 11),\n",
       " (0.9450029000070751, 9),\n",
       " (1.5902350676874262, 8),\n",
       " (1.7001698887611496, 4),\n",
       " (2.0279037242924214, 10),\n",
       " (2.6029487124670974, 5),\n",
       " (2.9233854674555144, 7),\n",
       " (3.8761701176232863, 12)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(abs(w), ind) for ind, w in enumerate(model.weights.toArray())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model to the `output/linearmodel` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(sc, \"output/linearmodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read the model like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LinearRegressionModel\n",
    "model_loaded = LinearRegressionModel.load(sc, \"output/linearmodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweaking the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent has several hyper-parameters that need to be tuned to obtain the best results. For example, the step-size parameter helps to stabilize the gradient descent algorithm. If it’s too small, the algorithm will take too many small steps to converge. If it’s too large, the algorithm may never converge. The right value depends on the dataset.\n",
    "It’s similar with the number of iterations. If it’s too large, fitting the model will take too much time. If it’s too small, the algorithm may not reach the minimum.\n",
    "\n",
    "Spark will iterate until the algorithm converges and will determine the optimal number iterations on its own. But you have to find the optimal value for the step size parameter yourself.\n",
    "\n",
    "The following function will help you experiment with several combinations of these parameters and find the one with best results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterateLRwSGD(iterArray, stepSizeArray, trainRdd, testRdd):\n",
    "    for numIter in iterArray:\n",
    "        for step in stepSizeArray:\n",
    "            alg = LinearRegressionWithSGD.train(trainRdd, iterations=numIter, step=step, intercept=True)\n",
    "            rescaledPredicts = trainRdd.map(lambda x: (alg.predict(x.features), x.label))\n",
    "            validPredicts = testRdd.map(lambda x: (alg.predict(x.features), x.label))\n",
    "            meanSquared = math.sqrt(rescaledPredicts.map(lambda pl: math.pow(pl[0]-pl[1], 2)).mean())\n",
    "            meanSquaredValid = math.sqrt(validPredicts.map(lambda pl: math.pow(pl[0]-pl[1], 2)).mean())\n",
    "            print(\"%d, %5.3f -> %.4f, %.4f\" % (numIter, step, meanSquared, meanSquaredValid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function will take several values for the number of iterations and step sizes, and two RDDs with train and test data, and will return the RMSE of training and validation sets for each combination of input parameters.\n",
    "\n",
    "Try it out on our `trainScaled` and `validScaled` RDDs, with \n",
    "600 iterations and with step sizes of 0.05, 0.1, 0.5, 1, 1.5, 2 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600, 0.050 -> 7.2564, 6.4885\n",
      "600, 0.100 -> 5.4933, 4.7533\n",
      "600, 0.500 -> 4.8589, 4.3512\n",
      "600, 1.000 -> 4.8112, 4.3353\n",
      "600, 1.500 -> 4.7930, 4.3249\n",
      "600, 2.000 -> 4.7873, 4.3213\n",
      "600, 3.000 -> 700170309.7761, 653469245.7477\n"
     ]
    }
   ],
   "source": [
    "iterateLRwSGD([600], [0.05, 0.1, 0.5, 1, 1.5, 2, 3], trainScaled, validScaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see several things from this output. First, the testing RMSE is always greater than the training RMSE (except for some corner cases). That’s to be expected. Furthermore, both errors decline rapidly as step size increases, following some inverse exponential function. That makes sense because for smaller smaller step sizes, there weren’t enough iterations to get to the minimum.\n",
    "\n",
    "Then the error values flatten out. This also makes sense because there are some limitations to how well you can fit a dataset. For a step size value of 3, the error values explode. This step size value is too large, and the\n",
    "algorithm misses the minimum. It seems that a step size near 1.0 gives the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding higher-order polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, data doesn’t follow a simple linear formula (a straight line in a two-dimensional space) but may be some kind of a curve. Curves can often be described with functions containing higher-order polynomials.\n",
    "\n",
    "Spark doesn’t offer a method of training a nonlinear regression model that includes higher-order polynomials, such as the preceding hypothesis. Instead, you can employ a little trick and do something that has a similar effect: you can expand your dataset with additional features obtained by multiplying the existing ones.\n",
    "\n",
    "You can use this method for doing that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addHighPols(v):\n",
    "    v1 = [[x, x*x] for x in v.toArray()]\n",
    "    return Vectors.dense([x for l in v1 for x in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use it to transform features from the `housingData` RDD and create a new RDD called `housingHP` containing `LabeledPoint` objects with the transformed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "housingHP = housingData.map(lambda x: LabeledPoint(x.label, addHighPols(x.features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the first transformed `LabeledPoint` and see how many features it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(housingHP.first().features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split and scale the new dataset as you did previously, cache the resulting RDDs and call them `trainHPScaled` and `validHPScaled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "setsHP = housingHP.randomSplit([0.8, 0.2])\n",
    "housingHPTrain = setsHP[0]\n",
    "housingHPValid = setsHP[1]\n",
    "featuresHPTrain = housingHPTrain.map(lambda lp: lp.features)\n",
    "featuresHPValid = housingHPValid.map(lambda lp: lp.features)\n",
    "scalerHP = StandardScaler(True, True).fit(featuresHPTrain)\n",
    "trainHPScaled = housingHPTrain.map(lambda lp: lp.label).zip(scalerHP.transform(featuresHPTrain)).\\\n",
    "  map(lambda lft: LabeledPoint(lft[0], lft[1])).cache()\n",
    "validHPScaled = housingHPValid.map(lambda lp: lp.label).zip(scalerHP.transform(featuresHPValid)).\\\n",
    "  map(lambda lft: LabeledPoint(lft[0], lft[1])).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `iterateLRwSGD` on these new RDDs with 600 iterations and these step size values: 0.4, 0.5, 0.6, 0.7, 0.9, 1.0, 1.1, 1.2, 1.3, 1.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600, 0.400 -> 4.7596, 4.1820\n",
      "600, 0.500 -> 4.6618, 4.1159\n",
      "600, 0.600 -> 4.5731, 4.0599\n",
      "600, 0.700 -> 4.4938, 4.0125\n",
      "600, 0.900 -> 4.3642, 3.9419\n",
      "600, 1.000 -> 4.3130, 3.9169\n",
      "600, 1.100 -> 4.2679, 3.8966\n",
      "600, 1.200 -> 4.2378, 3.8901\n",
      "600, 1.300 -> 64.8313, 55.5298\n",
      "600, 1.500 -> 52494653.3600, 52545943.4764\n"
     ]
    }
   ],
   "source": [
    "iterateLRwSGD([600], [0.4, 0.5, 0.6, 0.7, 0.9, 1.0, 1.1, 1.2, 1.3, 1.5], trainHPScaled, validHPScaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the results, the RMSE explodes for a step size of 1.3, and you get the best results for a step\n",
    "size of 1.1 or 1.2. The error values are lower than before. We can conclude that adding higher-order polynomials helped the linear-regression algorithm find a better-performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso and Ridge regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add regularization to linear regression training in Spark with Lasso and Ridge regressions. These two methods can help with training such models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import RidgeRegressionWithSGD\n",
    "from pyspark.mllib.regression import LassoWithSGD\n",
    "\n",
    "def iterateRidge(iterNums, stepSizes, regParam, train, test):\n",
    "    for numIter in iterNums:\n",
    "        for step in stepSizes:\n",
    "            model = RidgeRegressionWithSGD.train(train, iterations=numIter, regParam=regParam, step=step, intercept=True)\n",
    "            rescaledPredicts = train.map(lambda x: (model.predict(x.features), x.label))\n",
    "            validPredicts = test.map(lambda x: (model.predict(x.features), x.label))\n",
    "            meanSquared = math.sqrt(rescaledPredicts.map(lambda pl: math.pow(pl[0]-pl[1], 2)).mean())\n",
    "            meanSquaredValid = math.sqrt(validPredicts.map(lambda pl: math.pow(pl[0]-pl[1], 2)).mean())\n",
    "            print(\"%d, %5.3f -> %.4f, %.4f\" % (numIter, step, meanSquared, meanSquaredValid))\n",
    "\n",
    "def iterateLasso(iterNums, stepSizes, regParam, train, test):\n",
    "    for numIter in iterNums:\n",
    "        for step in stepSizes:\n",
    "            modell = LassoWithSGD.train(train, iterations=numIter, step=step, regParam=regParam, intercept=True)\n",
    "            rescaledPredicts = train.map(lambda x: (modell.predict(x.features), x.label))\n",
    "            validPredicts = test.map(lambda x: (modell.predict(x.features), x.label))\n",
    "            meanSquared = math.sqrt(rescaledPredicts.map(lambda pl: math.pow(pl[0]-pl[1], 2)).mean())\n",
    "            meanSquaredValid = math.sqrt(validPredicts.map(lambda pl: math.pow(pl[0]-pl[1], 2)).mean())\n",
    "            print(\"%d, %5.3f -> %.4f, %.4f\" % (numIter, step, meanSquared, meanSquaredValid))\n",
    "            print(\"\\tweights: \"+str(model.weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these methods add another parameter (the third one): the regularization parameter. Try them out with the same RDDs, with 600 iterations, the same step size and the regularization parameter of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600, 1.200 -> 4.3440, 3.9373\n",
      "600, 1.200 -> 4.2369, 3.8730\n",
      "\tweights: [-0.7378968231306785,0.7266956603940284,-0.24366959965218268,0.5965668065825196,-1.7001698887611496,2.6029487124670974,-0.2098393251764806,-2.9233854674555144,1.5902350676874262,-0.9450029000070751,-2.0279037242924214,0.7628197291548122,-3.8761701176232863]\n"
     ]
    }
   ],
   "source": [
    "iterateRidge([600], [1.2], 0.01, trainHPScaled, validHPScaled)\n",
    "iterateLasso([600], [1.2], 0.01, trainHPScaled, validHPScaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, for this dataset regularization doesn't help much. It would be different if you had a problem with overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your dataset is very large so that it doesn't fit into memory or you want to speed up the training, you can use *mini-batch stochastic gradient descent* optimizer. Instead of going through all the data points in each iteration, it will consider only some batch of data. It has more trouble converging, but is much more resource efficient.\n",
    "\n",
    "This function will allow you to test mini-batch SGD with several combinations of iterations, step sizes and mini-batch fractions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterateLRwSGDBatch(iterNums, stepSizes, fractions, train, test):\n",
    "    for numIter in iterNums:\n",
    "        for step in stepSizes:\n",
    "            for mbf in fractions:\n",
    "                model = LinearRegressionWithSGD.train(train, iterations=numIter, step=step, \n",
    "                                                      miniBatchFraction=mbf, intercept=True)\n",
    "                rescaledPredicts = train.map(lambda x: (model.predict(x.features), x.label))\n",
    "                validPredicts = test.map(lambda x: (model.predict(x.features), x.label))\n",
    "                meanSquared = math.sqrt(rescaledPredicts.map(lambda pl: math.pow(pl[0]-pl[1], 2)).mean())\n",
    "                meanSquaredValid = math.sqrt(validPredicts.map(lambda pl: math.pow(pl[0]-pl[1], 2)).mean())\n",
    "                print(\"%d, %5.3f %5.3f -> %.4f, %.4f\" % (numIter, step, mbf, meanSquared, meanSquaredValid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it now using `trainHPScaled` and `validHPScaled` RDDs, with 600 iterations, step sizes of 0.1, 0.2, 0.3, 0.4, 0.5, and mini-batch fractions of 0.01, 0.1, 0.3, 0.5, 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600, 0.100 0.010 -> 6.2449, 5.6129\n",
      "600, 0.100 0.100 -> 5.3266, 4.7609\n",
      "600, 0.100 0.300 -> 5.4031, 4.8421\n",
      "600, 0.100 0.500 -> 5.3769, 4.8068\n",
      "600, 0.100 0.800 -> 5.4362, 4.9046\n",
      "600, 0.200 0.010 -> 7.0249, 6.3063\n",
      "600, 0.200 0.100 -> 4.7465, 4.1597\n",
      "600, 0.200 0.300 -> 4.8229, 4.2129\n",
      "600, 0.200 0.500 -> 4.8854, 4.2282\n",
      "600, 0.200 0.800 -> 4.9318, 4.2925\n",
      "600, 0.300 0.010 -> 5.5822, 5.0739\n",
      "600, 0.300 0.100 -> 4.6132, 4.0256\n",
      "600, 0.300 0.300 -> 4.6698, 4.1001\n",
      "600, 0.300 0.500 -> 4.7343, 4.1451\n",
      "600, 0.300 0.800 -> 4.8012, 4.1714\n",
      "600, 0.400 0.010 -> 18.7670, 18.5032\n",
      "600, 0.400 0.100 -> 4.5220, 3.9502\n",
      "600, 0.400 0.300 -> 4.5236, 4.0198\n",
      "600, 0.400 0.500 -> 4.6039, 4.1153\n",
      "600, 0.400 0.800 -> 4.6716, 4.1054\n",
      "600, 0.500 0.010 -> 474.3977, 454.9575\n",
      "600, 0.500 0.100 -> 5.2657, 4.5342\n",
      "600, 0.500 0.300 -> 4.5402, 4.1871\n",
      "600, 0.500 0.500 -> 4.6386, 4.1886\n",
      "600, 0.500 0.800 -> 4.5032, 4.0003\n"
     ]
    }
   ],
   "source": [
    "iterateLRwSGDBatch([600], [0.1, 0.2, 0.3, 0.4, 0.5], [0.01, 0.1, 0.3, 0.5, 0.8], trainHPScaled, validHPScaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the mini-batch fraction significantly influences the best performance you can get, but that is still worse than the classic SGD. If you have lots of data, that difference might be worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, LBFGS algorithm can computationaly outperform all of the algorithms we've seen so far. LBFGS is a limited-memory approximation of the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm for minimizing multidimensional functions. \n",
    "\n",
    "Unfortunatelly however, LBFGS algorithm for linear regression is not available in Python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
