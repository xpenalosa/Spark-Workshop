{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first initialize a Spark context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(master='local[*]', appName=\"Spark course\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now pretty-print the current Spark configuration using `sc.getConf` (you can use [Spark documentation](https://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.SparkConf) as a reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.memory.offHeap.size', '4g')\n",
      "('spark.jars', 'file:///usr/local/spark/python/axs/AxsUtilities-1.0-SNAPSHOT.jar,file:///home/spark/first-edition/ch06/kafkaProducerWrapper.jar,file:///home/spark/.ivy2/jars/org.mariadb.jdbc_mariadb-java-client-2.2.3.jar')\n",
      "('spark.driver.port', '40074')\n",
      "('spark.app.name', 'Spark course')\n",
      "('spark.files', 'file:///home/spark/.ivy2/jars/org.mariadb.jdbc_mariadb-java-client-2.2.3.jar')\n",
      "('spark.jars.packages', 'org.mariadb.jdbc:mariadb-java-client:2.2.3')\n",
      "('spark.local.dir', '/home/spark/sparktmp')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.driver.host', '10.0.2.15')\n",
      "('spark.driver.memory', '1g')\n",
      "('spark.ui.killEnabled', 'true')\n",
      "('spark.memory.offHeap.enabled', 'true')\n",
      "('spark.sql.warehouse.dir', 'file:///home/spark/spark-warehouse')\n",
      "('spark.scheduler.minRegisteredResourcesRatio', '0.75')\n",
      "('spark.executor.extraJavaOptions', '-XX:MaxDirectMemorySize=4096m')\n",
      "('spark.executor.memory', '1g')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.repl.local.jars', 'file:///usr/local/spark/python/axs/AxsUtilities-1.0-SNAPSHOT.jar,file:///home/spark/first-edition/ch06/kafkaProducerWrapper.jar,file:///home/spark/.ivy2/jars/org.mariadb.jdbc_mariadb-java-client-2.2.3.jar')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.files.maxPartitionBytes', '471859200')\n",
      "('spark.driver.extraJavaOptions', '-Dderby.system.home=/home/spark/metastore_db')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.submit.pyFiles', '/home/spark/.ivy2/jars/org.mariadb.jdbc_mariadb-java-client-2.2.3.jar')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.app.id', 'local-1564753940971')\n",
      "('spark.default.parallelism', '4')\n",
      "('spark.ui.showConsoleProgress', 'true')\n"
     ]
    }
   ],
   "source": [
    "for c in sc.getConf().getAll():\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Spark's current parallelism level? (Hint: the parameter is called \"spark.default.parallelism\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(sc.getConf().get(\"spark.default.parallelism\"))\n",
    "print(sc.defaultParallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the corresponding page at [Spark Web UI](http://192.168.10.2:4040/environment/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Spark `NOTICE` file (from `/usr/local/spark` folder) into a value called `noticeRdd`. (You might want to check out the [official documentation for SparkContext](https://spark.apache.org/docs/2.4.0/api/python/pyspark.html#pyspark.SparkContext))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "noticeRdd = sc.textFile(\"/usr/local/spark/NOTICE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How many lines does the NOTICE file have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1174"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noticeRdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an rdd called `words` containing only words from the NOTICE file (word is any string of symbols separated by whitespace).   \n",
    "   \n",
    "   How many words does the NOTICE file have? (Notice: make sure not to count empty words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4895"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = noticeRdd.flatMap(lambda line: line.split(\" \")).filter(lambda word: word.strip() != \"\")\n",
    "words.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many DISTINCT words does the NOTICE file have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1512"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What is the average word length in the NOTICE file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.510316649642486"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.map(lambda word: len(word)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many words have length less than 3, how many are between 3 and 8 and how many are above? (Hint: use `histogram`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 3, 8, 1000], [746, 2494, 1655])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.map(lambda word: len(word)).histogram([0, 3, 8, 1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What is the total number of non-whitespace characters in the NOTICE file (Hint: use the `words` RDD and `reduce` action)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36763"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.map(lambda w: len(w)).reduce(lambda w1, w2: w1 + w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save distinct words from the NOTICE file to the `/home/spark/output/noticeWords` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.saveAsTextFile(\"/home/spark/output/noticeWords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your Linux shell to examine the output \"file\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visit [Spark's Web UI](http://192.168.10.2:4040) and examine jobs, stages and tasks that were executed when the previous cells were running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using accumulators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulators are variables shared across executors that you can only add to. You can use them to implement global sums and counters in your Spark jobs. Reading from accumulators is allowed only from the driver side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two accumulators `totallen` and `totalcount` and initialize them to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "totallen = sc.accumulator(0)\n",
    "totalcount = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `foreach` method to calculate the total length of all words and the count of the words and put those values into the two accumulators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_words(word):\n",
    "    totallen.add(len(word))\n",
    "    totalcount.add(1)\n",
    "    \n",
    "words.foreach(calc_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the accumulator values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36763, 4895)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totallen.value, totalcount.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the accumulator variables to calculate the average word length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.510316649642492"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totallen.value / float(totalcount.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using broadcast variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, variables created in the driver, needed by tasks for their execution, are serialized and shipped along with those tasks. But a single driver program can reuse the same variable in several jobs, and several tasks may get shipped to the same executor as part of the same job. So, a potentially large variable may get serialized and transferred over the network more times than necessary. In these cases, itâ€™s better to use broadcast variables, because they can transfer the data in a more optimized way and only once.\n",
    "\n",
    "Let's say you have the following dictionary of words that need to be corrected (execute the following cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "changedict = {\"Guice\": \"Juice\", \"Glyphicons\": \"Glyphs\", \"Bootstrap\": \"bootstrap\", \"Bengtson\": \"Benson\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcast this dictionary to the executors and use it inside the `mapPartitions` method to change all the matching words in the `words` RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictb = sc.broadcast(changedict)\n",
    "newwords = words.map(lambda w: dictb.value[w] if w in dictb.value else w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if you made any changes by counting some of the matching words in both new and the old RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(words.filter(lambda x: x == \"Guice\").count())\n",
    "print(words.filter(lambda x: x == \"Juice\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(newwords.filter(lambda x: x == \"Guice\").count())\n",
    "print(newwords.filter(lambda x: x == \"Juice\").count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
